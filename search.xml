<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ResNeXt:Aggregated Residual Transformations for Deep Neural Networks]]></title>
    <url>%2F2019%2F03%2F26%2Fresnext%2F</url>
    <content type="text"><![CDATA[介绍 文章提出了ResNext模型，其结合了Resnet和Inception结构的思想，通过聚合一系列结构相同的块，达到了以更少的参数获得更佳的性能。其中引入了一个新的维度来描述模型复杂程度，Cardinality,指每个处理单元中拓扑结构相同的块的个数，ResNext中Next指的就是这个next dimension。相较于增加模型的深度和宽度，增加模型的cardinality是一种更有效地增加模型表达能力的方式。ResNext模型最终获得ILSVRC-2016上分类任务第二名。 ResNext ResNext基本块结构如下： 左边为ResNet，右边为ResNext，参考了Inception多分支结构，但不同分支拓扑结构相同。二者具有大致相同的参数量和计算复杂度。 ResNext块有如下等价形式，实现中采用第三种方式： 其中每一层标记为输入通道，卷积大小，输出通道。在相应通道维度进行拼接可见以上三种形式是等价的。 近似复杂度的cardinality与width关系如下： 堆叠而成的ResNext-50 结构如下表： 其中，ResNext-50与ResNet-50参数量和计算复杂度相近。 Experiments Cardinality vs Width 增加基本块内的cardinality比增加块内width更有效，如下表所示(保持复杂度)： Increasing Cardinality vs Deeper/Wider 增加cardinality也比加深或加宽网络有更有效的表示能力，32x4d版ResNext-101甚至比其多一倍复杂度的ResNext-200结果还要好。 Residual connections 残差连接也很重要： performance 与几个优秀模型在ImageNet数据集上的比较： 参考文献 Xie, S., Girshick, R., Dollár, P., Tu, Z., & He, K. (2017). Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 1492-1500).]]></content>
      <categories>
        <category>computer vision</category>
        <category>classification</category>
      </categories>
      <tags>
        <tag>classification</tag>
        <tag>CVPR2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Inception-v4,Inception-ResNet and the Impact of Residual Connections on Learning]]></title>
    <url>%2F2019%2F03%2F25%2Finception-v4%2F</url>
    <content type="text"><![CDATA[介绍 文章提出了更复杂的Inception结构，并与ResNet进行了结合，最终集成模型获得ImageNet分类任务top-5 error为3.08%。 Inception-V4 Inception-V4结构如下： 其中对应Inception-A,Inception-B,Inception-C如下： Inception-ResNet Inception-ResNet结构如下： 其中对应Inception-ResNet-V1版本Inception-A,Inception-B,Inception-C如下： 其中对应Inceptipn-ResNet-V2版本Inception-A,Inception-B,Inception-C如下： Result 不同版本Inception网络在ImageNet运行结果如下： 参考文献 Szegedy, C., Ioffe, S., Vanhoucke, V., & Alemi, A. A. (2017, February). Inception-v4, inception-resnet and the impact of residual connections on learning. In Thirty-First AAAI Conference on Artificial Intelligence.]]></content>
      <categories>
        <category>computer vision</category>
        <category>classification</category>
      </categories>
      <tags>
        <tag>classification</tag>
        <tag>AAAI 2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Inceptionv3:Rethinking the inception architecture for computer vision]]></title>
    <url>%2F2019%2F03%2F25%2Finception-v3%2F</url>
    <content type="text"><![CDATA[介绍 文章旨在通过合理地设计Inception结构达到更有效地利用增加的计算来获得更好地性能，在ILSVRC分类任务上获得17.3%的top-1 error及3.5% top-5 error。 General Design Principles 为有效扩展卷积网络地表示能力，文章利用Inception模块中存在的dimension reduction和parallel structures特性，给出了几个通用的设计原则： 避免表示瓶颈，尤其在网络的初始层。表示维度应该逐步地从输入递减到输出。 高维特征表达通常有更强的区分能力，带来更快的训练速度。 空间聚合在低维表示处进行将不会有太多的表示能力损失。 需要平衡网络的深度与宽度。 Factorizing Convolutions 分解为小的卷积 大的卷积核可以分解为多个小卷积的堆叠，如5x5卷积与两个堆叠3x3卷积有相同的感受野, 但使用小的卷积核有更少的参数量同时又不损失表示能力。 可将原始Inception结构中使用的5x5卷积替换为堆叠的两个3x3卷积。 分解为非对称的卷积 相较于分解为一系列3x3卷积，分解为非对称的卷积会有更少的参数量。如3x3卷积可分解为3x1卷积后接1x3卷积。 n x n卷积可分解为1 x n卷积后接n x 1卷积。 Efficient Grid Size Reduction 通常通过1x1卷积调整特征通道维度，通过池化减小feature map，如输入为\(d \times d \times k\)，输出为\(\frac{d}{2} \times \frac{d}{2} \times 2k\)可组合1x1卷积和池化完成。如先池化再卷积比先卷积再池化虽然有更少的参数，但会损失模型表示能力： 实际上可以通过以下方式来更好地解决表示瓶颈，同时也拥有较少的参数： Inception-V3 Inception V3结构如下表： 参考文献 Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., & Wojna, Z. (2016). Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2818-2826).]]></content>
      <categories>
        <category>computer vision</category>
        <category>classification</category>
      </categories>
      <tags>
        <tag>classification</tag>
        <tag>CVPR2016</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SENet:Squeeze-and-Excitation Networks]]></title>
    <url>%2F2019%2F03%2F24%2Fsenet%2F</url>
    <content type="text"><![CDATA[介绍 文章聚焦在channel之间特征相关性来提升深度网络的表达能力，提出了Squeeze-and-Excitation block来对不同channel的特征进行重标定。文章使用SENet结构获得了ILSVRC-2017分类任务的冠军，在ImageNet数据集上top-5 error低至2.251%。 基本的SE block如下所示： 对于网络中的变换\(F_{tr}: X \to U, X \in \mathbb{R}^{H&#39; \times W&#39; \times C&#39;}, U \in \mathbb{R}^{H \times W \times C}\)，我们可以构造SE block来进行特征标定。首先把特征\(U\)通过一个squeeze操作，即把feature map在\(H \times W\)上进行聚合得到一个channel descriptor。此channel descriptor包含了channel-wise特征响应的全局分布，能够使得全局感受野的信息被网络的低层所利用。然后执行excitation操作，对每个channel通过self-gating机制生成权重，最终使用不同channel的权重对原始U进行处理，把得到的结果替代U作为后续层的输入。 SENet可由许多SE Block堆叠而成，SE Block可以在原始网络的任意深度处作就地替换。在浅层中SE Block更多了excite类别无关信息，在深层中逐渐特化。 Squeeze-and-Excitation Blocks 对于网络中的变换\(F_{tr}: X \to U, X \in \mathbb{R}^{H&#39; \times W&#39; \times C&#39;}, U \in \mathbb{R}^{H \times W \times C}\)，我们简化\(F_{tr}\)为一系列卷积操作。命\(V = [v_1, v_2, ...v_C]\)表示对应的卷积核，\(F_{tr}\)的输出\(U = [u_1, u_2, ...u_C]\)，其中 \[\begin{equation} u_c = v_c * X = \sum_{s=1}^{C&#39;} v_c^s * x^s \end{equation}\] 其中*表示卷积操作，\(v_c = [v_c^1, v_c^2, ..., v_c^{C&#39;}], X = [x^1, x^2,...,x^{C&#39;}]\)。由上式可见输出是由输入的每一个channel处都参与summation得到,因此已隐含有channel依赖信息，但此信息与空间位置混在一起。文章的目的在于提升网络对于有用特征的灵敏度，同时抑制无用信息，通过squeeze和excitation操作来显式地建模不同通道特征之间相关性，进而进行特征标定。 Squeeze: Global Information Embedding 由于原卷积操作只作用于局部感受野，不能利用全局上下文信息，因此首先使用squeeze操作来将全局位置信息编码入channel descriptor。这是通过全局平均池化的方式来得到\(z \in \mathbb{R}^C\)，其中第c个元素为： \[\begin{equation} z_c = F_{sq}(u_c) = \frac{1}{H \times W} \sum_{i=1}{H}\sum{j=1}{W}u_c(i, j) \end{equation}\] Excitation: Adaptive Recalibration 然后使用excitation捕获channel-wise依赖： \[\begin{equation} s = F_{ex}(z, W) = \sigma(g(z, W)) = \sigma(W_2\sigma(W_1z)) \end{equation}\] 其中\(\sigma\)表示Relu函数，\(W_1 \in \mathbb{R}^{\frac{C}{r} \times C}, W_2 \in \mathbb{R}^{C \times \frac{C}{r}}\),即通过两个全连接层来学习非线性权重，其中一个以比率\(r\)降低维度，另一个进行还原。最终标定后输出为： \[\begin{equation} \widetilde{x}_c = F_{scale}(u_c, s_c) = s_c \cdot u_c \end{equation}\] 其中\(\widetilde{X} = [\widetilde{x}_1, \widetilde{x}_2, ..., \widetilde{x}_C]\), \(F_{scale}(u_c, s_c)\)为在特征图\(u_c \in \mathbb{R}^{H \times W}\)与标量\(s_c\)之间channel-wise乘法。 SENet可由SE Block对原网络结构改造而成，如ResNet残差单元及Inception模块修改后基本结构如下： SENet SE-ResNet-50及SE-ResNext-50整体组成单元见下表： 可见仅在原残差块内增加2个全连接层,额外引入参数数量为： \[\begin{equation} \frac{2}{r}\sum_{s=1}^{S}N_s \cdot C_s^2 \end{equation}\] 其中r为reduction ratio，\(S\)为不同feature map大小的阶段数目，\(C_s\)为第\(s\)个阶段的输入特征通道维度，\(N_s\)为每个阶段的重复残差块数量。 在ImageNet数据集上运行结果如下： 参考文献 Hu, J., Shen, L., & Sun, G. (2018). Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 7132-7141).]]></content>
      <categories>
        <category>computer vision</category>
        <category>classification</category>
      </categories>
      <tags>
        <tag>classification</tag>
        <tag>CVPR2018</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Networks with Stochastic Depth]]></title>
    <url>%2F2019%2F03%2F23%2Fstochastic-depth%2F</url>
    <content type="text"><![CDATA[介绍 较浅的神经网络表示能力不如较深的网络，但较深的网络相对难以训练，文章提出了stochastic depth方式达到了在训练时使用较浅的网络及在测试时使用较深的网络的效果，最终能大幅减少训练时间并获得性能上的提升。 测试时错误率的降低可归功于两点：一是缩短了期望的训练网络深度使得前向传播和梯度回传更加便捷，二是此种方式训练可以看作很多不同深度的网络模型进行了集成，可以有效地防止过拟合。 Stochastic Depth ResNet architecture ResNet基本残差单元如下： \[\begin{equation} H_l = ReLU(f_l(H_{l-1}) + id(H_{l-1})) \end{equation}\] 其中\(H_l\)是第l层的输出，\(f_l\)是由Conv, Batch Norm堆叠的层，如下图所示： Stochastic depth 文章在使用resnet block网络结构基础上，使用stochastic depth减少训练时网络的实际深度，具体而言就是通过随机地以一定概率直接丢弃整块残差单元，然后把相邻块的信息直接相连，此时有： \[\begin{equation} H_l = ReLU(b_lf_l(H_{l_1}) + id(H_{l-1})) \end{equation}\] 其中\(b_l\)为伯努利分布随机变量，\(p_l = Pr(b_l=1)\)表示第\(l\)个Resnet Block被保留的概率，如果\(b_l = 1\)则退化到原始Resnet形式，如\(b_l = 0\) 则相当于identity function， \[\begin{equation} H_l = id(H_{l-1}) \end{equation}\] The survival probabilities \(p_l\)为新引入的模型参数，直观地相邻层的\(p_l\)值应该很接近。实际上由于浅层用来提取特征并被深层使用，因此浅层被保留的概率应该更大。文章探讨了两种方式，一是设置\(p_l = p_L\)，对所有层有均匀的保留概率，好处是只引入一个参数；另外一种方式是线性地衰减保留概率，如初始层\(p_0 = 1\)，最深层为\(p_L\)，则中间层\(l\)被保留的概率为： \[\begin{equation} p_l = 1 - \frac{l}{L}(1-p_L) \end{equation}\] Stochastic depth网络结构如下图： Expected network depth 在训练中模型的实际有效深度为\(\hat{L}\)，则其期望值\(E(\hat{L}) = \sum_{l=1}^{L}p_l\)。在\(p_L = 0.5\)的线性衰减策略下，\(E(\hat{L}) = (3L-1)/4\)，对于文章使用的110层的网络，其\(L=54\)，实际训练的有效深度约为40，这能显著缓解深层网络中的梯度消失与信息丢失问题。 Training time savings 当一个残差单元被drop时，就不需要为其进行前向及后向传播中的计算，这将大幅提升训练速度。对于上述\(p_L=0.5\)的线性衰减网络而言，将节省约25%的训练时间。 Implicit model ensemble 以stochastic depth方式训练时相当于将不同深度的网络模型进行集成。对于\(L\)个残差块的网络就有\(2^L\)种不同的模型组合可能，每次训练一个batch时就从这\(2^L\)中可能中选择一个网络，在测试时则对所有的网络进行平均。 Stochastic depth during testing 在测试时没有残差单元被丢弃，此时根据每个残差块参与训练的期望程度进行计算，测试时的前向传播如下： \[\begin{equation} H_l^{Test} = ReLU(p_lf_l(H_{l-1}^{Test};W_l) + H_{l-1}^{Test}) \end{equation}\] 参考文献 Huang, G., Sun, Y., Liu, Z., Sedra, D., & Weinberger, K. Q. (2016, October). Deep networks with stochastic depth. In European conference on computer vision (pp. 646-661). Springer, Cham.]]></content>
      <categories>
        <category>computer vision</category>
        <category>classification</category>
      </categories>
      <tags>
        <tag>classification</tag>
        <tag>ECCV2016</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Wide Residual Networks]]></title>
    <url>%2F2019%2F03%2F23%2Fwrn%2F</url>
    <content type="text"><![CDATA[介绍 文章提出了通过扩展神经网络的宽度而不是深度也可以获得良好的性能，同时验证了残差单元中使用dropout的有效性。对于卷积神经网络而言，宽度指的是在每一层中使用的卷积核的数目，即通道这一维度的数目。 文章使用Resnet作为基本的参考网络结构，为了提高深度模型的表示能力，可以在每个残差单元中加入更多的层，扩大卷积层中卷积核的大小或扩宽卷积核。 文章使用以下符号标记： l: deepening factor，每个残差单元内卷积层的数量，对于basic版本Resnet是2，bottleneck版本是3。 k: widdening factor，每个卷积层处卷积核的数目是原始的多少倍，原始的网络结构都是1。 d: number of blocks, 基本残差单元的数量, 网络正是由一系列基本的残差单元堆叠而成。 B(M): B表示一个残差单元，M为此残差单元内的一系列卷积层，卷积核都是方块，残差单元内的卷积层都有相同的通道数。如B(3,3)就是原始basic版本Resnet残差单元，B(1,3,1)是对原始bottleneck版本Resnet残差单元的一个加强。(bottneck先1x1降维后1x1还原维度) n: 网络中全部卷积层的数量。WRN-n-k表示一共有n层，widdening factor为k。 文章通过实验发现网络层有相近数量的参数时模型输出有相近的性能。在网络有不同的深度时加宽网络能连续性地提升性能，同时加深网络和加宽网络会带来更多的参数进而需要更多的数据来防止拟合，同时在残差单元内的卷积层之间使用dropout是有效的，尽管已经使用了Batch Normalization。文章显示了使用16层加宽的网络能够获得媲美1000层的未加宽的性能，同时能够加快训练速度。 参考文献 Zagoruyko, S., & Komodakis, N. (2016). Wide residual networks. arXiv preprint arXiv:1605.07146.]]></content>
      <categories>
        <category>computer vision</category>
        <category>classification</category>
      </categories>
      <tags>
        <tag>classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DenseNet:Densely Connected Convolutional Networks]]></title>
    <url>%2F2019%2F03%2F23%2Fdensenet%2F</url>
    <content type="text"><![CDATA[介绍 文章提出了DenseNet模型，其中Dense指神经网络层与层之间的连接。在常规的卷积神经网络中，L层的网络共有L个连接，在DenseNet中L层的网络有\(\frac{L(L+1)}{2}\)个连接。在feature map大小匹配的层中，每一层都把其前面所有层的feature map作为其输入，也把其自己的feature map作为后续所有层的输入，Dense Block如下图所示： DenseNet因其密集地连接方式，能够缓解梯度消失问题，强化特征传播与特征复用，事实上减少了参数数量。DenseNet中每一层都可以相对直接地获取从loss传来的梯度信号及原始输入信息，更好的信息与梯度流动让模型易于训练。DenseNet在CIFAR-10，CIFAR-100，SVHN，ImageNet等数据集上获得了state-of-the-art表现。 DenseNet DenseNet由一系列Dense Block组成，每一个Dense Block内具有相同大小的feature map，不同的feature map大小的Dense Block之间通过Transition layer来连接，如下图所示： Dense Block 对于ResNet的残差块有： \[\begin{equation} x_l = H_l(x_{l-1}) + x_{l-1} \end{equation}\] 其中\(x_{l-1}\)表示第\(l-1\)层的输入，H是由Convluation, Batch Normalization, Relu组合而成的pre-activation残差函数(堆叠顺序为BN-Relu-Conv)。 Dense Block沿用Resnet的设计，区别为其接受前面所有层的feature map作为输入： \[\begin{equation} x_l = H_l([x_0, x_1, ..., x_{l-1}]) \end{equation}\] 其中\([x_0, x_1, ..., x_{l-1}]\)表示将0到\(l-1\)层的feature map在通道维concat起来。 Transition Layer Transition layer 连接不同feature map大小的Dense Block，其包含batch normalization, 1x1 conv layer和 2x2 average pooling。 如果\(H_l\)产生k个feature map，则第l层将有\(k_0+k(l-1)\)个feature map作为输入，此处k为growth rate。DenseNet与此前的网络不同之处在于可以拥有非常narrow的层获得较好性能，如取k=12。 尽管每层只产生k个feature map，但其拥有非常多的输入feature map。同bottleneck版本resnet一样，可以先使用1x1卷积来降低维度提高计算效率，指定此种版本为DenseNet-B(BN-Relu-Conv(1x1)-BN-Relu-Conv(3x3))。同时也可以在transition layer降低维度，此种版本为DensetNet-C。 在ImageNet数据集上运行的类似Resnet结构的Densent组成如下表： 使用DenseNet在ImageNet运行实验结果如下表，可见Denset有更高的参数使用效率，如DenseNet-201参数与Resnet-34相仿性能却与Resnet-101相差无几。 总结 DenseNet允许学到的feature map被后续所有层高效地访问，导致很好的特征复用与紧凑的模型。其中loss回传的梯度信息只需要经过很少的transition layer，有更短的路径，这相当于一种Deep Supervision。在Stochastic depth网络中，随机drop一些layer后也相当于创建了不同layer的直接连接，这表明DenseNet也是一种有效的正则化方式。总而言之，DenseNet可以通过更少的参数与计算量获得更佳的性能。 参考文献 Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708).]]></content>
      <categories>
        <category>computer vision</category>
        <category>classification</category>
      </categories>
      <tags>
        <tag>classification</tag>
        <tag>CVPR2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Identity Mappings in Deep Residual Networks]]></title>
    <url>%2F2019%2F03%2F22%2Fidentity-mapping%2F</url>
    <content type="text"><![CDATA[介绍 文章是对原始ResNet模型的改进，使得在前向与后向传播中信息能够更好地流动，进一步解决了深度学习模型训练中的退化问题。ResNet中Identity Mapping只存在shortcut connection中，改进后残差单元间传递的也是Identity Mapping。 ResNet是由一系列的残差单元堆叠而成，如下所示： \[\begin{gather} y_l = h(x_l) + F(x_l, W_l),\\ x_{l+1} = f(y_l) \end{gather}\] 其中\(x_l\)是第\(l\)个残差单元的输入，\(W_l = \{W_{l,k} | 1 \leq k \leq K\}\)是第\(l\)个残差单元的权重，\(K\)是残差单元的层数(普通版\(K=2\),bottleneck版本\(K=3\))，\(F\)是残差函数，\(f\)是相加后的激活函数Relu，\(h\)是identity mapping时有\(h(x_l) = x_l\)。 如\(f\)也是identity mapping,将(2)代入(1)可得： \[\begin{equation} x_{l+1} = x_l + F(x_l, W_l) \end{equation}\] 迭代后对任意深层\(L\)和浅层\(l\)有： \[\begin{equation} x_{L} = x_l + \sum_{i= l}^{L-1}{F(x_i, W_i) } \end{equation}\] 可以看出，任意深层\(L\)可直接表示为任意浅层\(l\)加上一个残差函数，可以很方便地传递梯度： \[\begin{equation} \frac{\partial\epsilon}{\partial{x_l}} = \frac{\partial\epsilon}{\partial{x_L}}\frac{\partial{x_L}}{\partial{x_l}} = \frac{\partial\epsilon}{\partial{x_L}}\lgroup 1 + \sum_{i=l}^{L-1}{F(x_i, W_i)}\rgroup \end{equation}\] 其中\(\epsilon\)表示loss，公式(5)说明了对深层\(L\)的梯度可以直接传递到浅层\(l\)。 下图显示了原版ResNet及改进后的区别。 Identity Skip Connections 为探讨Identity Skip Connection的重要性，先假定\(f\)是identity，然后考察不同形式的skip connections。 对于取\(h(x_l) = \lambda_lx_l\)，有 \[\begin{equation} x_{l+1} = \lambda_lx_l + F(x_l, W_l) \end{equation}\] 迭代后对任意深层\(L\)和浅层\(l\)有： \[\begin{equation} x_{L} = (\prod_{i=L}^{L-1}\lambda_i)x_l + \sum_{i= l}^{L-1}{\hat{F}(x_i, W_i)} \end{equation}\] 其中\(\hat{F}\)为吸收了累乘因子的残差函数，进行求导有： \[\begin{equation} \frac{\partial\epsilon}{\partial{x_l}} = \frac{\partial\epsilon}{\partial{x_L}}\lgroup (\prod_{i=L}^{L-1}\lambda_i) + \frac{\partial}{\partial{x_l}}\sum_{i=l}^{L-1}{\hat{F}(x_i, W_i)}\rgroup \end{equation}\] 由于梯度中有\(\prod_{i=L}^{L-1}\lambda_i\)，当网络变深时容易出现梯度消失与梯度爆炸问题。 下图所示为不同的skip connections 及其结果。 Usage of Activation Functions 接下来探讨Identity在\(f\)方面的重要性，此时使用identity skip conntions，采用如下不同形式的残差块进行比较。 如上图(c)所示可实现naive版本的identity，但由于\(relu(x) = max(0, x)\)总是输出非负数，会影响残差块的表达能力。 上图(d),(e)的pre-activation指将本来处于相加后使用的relu放置在残差块的最前面，且只影响残差块这一分支。此时有： \[\begin{equation} x_{l+1} = x_l + F(\hat{f}(x_l), W_l) \end{equation}\] 形式如公式(4)，信息能够得到很好的传播。 事实上由于残差网络是由很多残差单元堆叠而成，after-addition activation和pre-activation是等价的，如下图所示： 关于不同\(f\)调整的结果见下表： 总结 本文提出了对ResNet结构的改进，使得信息能够更好地流通。主要是将原来会阻碍信息流动的addition之后的relu位置调到了残差函数的前面，使其不影响skip connection分支。 参考文献 He, K., Zhang, X., Ren, S., & Sun, J. (2016, October). Identity mappings in deep residual networks. In European conference on computer vision (pp. 630-645). Springer, Cham.]]></content>
      <categories>
        <category>computer vision</category>
        <category>classification</category>
      </categories>
      <tags>
        <tag>classification</tag>
        <tag>ECCV2016</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Batch Normalization:Accelerating Deep Neural Network Training by Reducing Internal Covariate Shift]]></title>
    <url>%2F2019%2F03%2F21%2Fbatchnorm%2F</url>
    <content type="text"><![CDATA[介绍 文章提出Batch Normalization，解决了网络中每层处理单元的输入分布随参数更新变化的问题，也即internal covariate shift，使得能够使用较大的学习率，不用过多关注初始化，减少了使用dropout的必要，能够加快网络的训练和获得更好的性能。 在训练神经网络时，浅层参数的更新会影响后续层的输入，导致后续层需要适应其输入分布的变化。对于sigmoid这类饱和激活函数而言，当其输入数据落入其饱和区时会导致梯度接近于0，参数更新变小，影响网络收敛速度。 通常我们使用白化操作对输入数据进行预处理，主要是PCA白化和ZCA白化，通过白化操作会去除特征之间的相关性，使得输入分布有相同的均值和方差，但对网络中的每一层数据都进行白化操作成本太高，因此提出了batch normalization，使得网络中每一层处理单元的输入有相同的分布。 Batch Normalization Batch normalization对同一batch内网络中的每一层的每一个特征，首先进行归一化至均值为0，标准差为1，对于d维输入$x = (x^{(1)}, x^{(2)}, ..., x^{(d)}) $ 有： \({\hat{x}}^{(k)} = \frac{ x^{(k)} - E[x^{(k)}] }{\sqrt{Var[x^{(k)}]}}\) 然后还原数据的表达能力： \({\hat{y}}^{(k)} = \gamma^{(k)} \hat{x}^{(k)} + \beta^{(k)}\) 其中\(\gamma^{(k)},\beta^{(k)}\)是第k处需要学习的一对参数。经过上述处理后，对应k处数据有稳定的均值为\(\beta^{(k)}\)，标准差为\(\gamma^{(k)}\)的分布。 最终将网络中每一层中原始的数据\(x^{(k)}\)替换为\(y^{(k)}\)。算法描述见下图： 对于全连接层，针对每个激活的神经元进行处理。对于卷积层，针对每一个feature map学习一对\(\beta^{(k)}\)，\(\gamma^{(k)}\)。 在测试时，网络中每一层的输入分布已经固定，其均值与标准差通过moving average的方式由训练时求得的\(\beta^{(k)}\)，\(\gamma^{(k)}\)计算而来，整体流程如下： 总结 Batch Normalization加快了训练速度，能够获取更好的性能。BN-Inception是增加了batch normaliztion的inception模型，也即使Inception-V2，在ImageNet上获得了top-5 error为4.82%。 参考文献 Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167.]]></content>
      <categories>
        <category>computer vision</category>
        <category>classification</category>
      </categories>
      <tags>
        <tag>classification</tag>
        <tag>ICML2015</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Resnet:Deep Residual Learning for Image Recognition]]></title>
    <url>%2F2019%2F03%2F19%2Fresnet%2F</url>
    <content type="text"><![CDATA[介绍 文章提出残差网络使得能够更容易地训练出更深层次的神经网络，Resnet模型给出了高至152层的网络，但却比VGG有更小的复杂度。Resnet模型横扫了ILSVRC-2015，COCO-2015上的分类、检测、分割等比赛任务的冠军。 更深的神经网络可能带来更好的性能，但却存在难以训练的退化问题，这并不是因为更深的网络导致了过拟合，而是训练误差都不能得到很好地下降，如下图所示： 退化问题说明了深度模型训练的困难。从理论分析来看，考虑一个浅层网络和在其之上加了更多层的深层网络，对于深层网络可以把额外增加的层训练为恒等映射，其余部分从浅层网络拷贝过来，那么对于深层网络就应该存在方案使得其训练误差可以比相应的浅层网络要小。实际训练网络的情况并非如此，本文使用残差学习解决这个问题。 Deep Residual Learning Residual Learning 假设\(H(x)\)是要通过堆叠的网络层学习的从输入到输出的映射，\(x\)表示输入，如果假定多层网络能够渐近地学习到\(H(x)\)，那么其也应该能学习到\(F(x)=H(x)-x\)。通过残差方式只需要学习\(F(x)\)，原始问题为\(F(x)+x\)，两种方式的学习难易程度却有很大的不同。 Identity Mapping by Shortcuts 残差学习可形式化地表达为\(y=F(x,{W_i})+x\)，其中\(x,y\)是当前整个块的输入和输出，\(F(x,{W_i})\)表示需要学习的残差映射。如上图所示为一个残差building block，包含两层，则\(F=W_2\sigma(W_1x)\)，其中\(\sigma\)表示Relu函数。由公式可以看出，残差方式没有带来额外的参数，也基本没有带来更多地运算，因此可以公平地与相应地plain network进行比较。当\(x\)与\(F\)有相同的维度时，直接进行element-wise相加即可，当维度不同时可对\(x\)投影到匹配的维度后再进行相加，此时有\(y=F(x,{W_i})+W_sx\)。当维度本就相同时也可使用方阵形式的\(W_s\)。 Network Architecture plain network plain network如同VGG一样堆叠3x3卷积网络，遵循两个设计原则： 对于输出的feature map大小一样的层有相同数量的卷积核。 如果输出的feature map大小减半，则卷积核的数量翻倍以保持每层的复杂度。 residual network residual network在前述的plain network上插入shortcut connection。当输入与输出维度相同时，可直接使用identity shortcut。当维度不同时，可仍选择shortcut mapping但进行额外补0以使维度匹配，或者使用projection shortcut。当shortcut connection跨越不同大小的feature map时，projection使用步长为2。 bottleneck 当配置的网络较深时存在参数过多的问题，因此可引入1x1的卷积核来降低维度减少计算量。具体来说，2层的3x3残差块中可由1x1,3x3,1x1堆叠的三层块代替，前面的1x1卷积用来降低维度，后面的1x1用来提升维度以使输入输出维度匹配，如下图所示： architecture 常用resnet有18，34，50，101及152层，其中50层及以上使用bottleneck版本的块。resnet首先使用步长为2的7x7卷积，然后包含四组不同feature map大小的堆叠的残差块，因此最终feature map的大小为输入的1/32。resnet中无全连接层，在最终的feature map上执行全局池化后输入softmax进行分类。不同深度的resnet配置如下表： 参考文献 He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).]]></content>
      <categories>
        <category>computer vision</category>
        <category>classification</category>
      </categories>
      <tags>
        <tag>classification</tag>
        <tag>CVPR2016</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GoogLeNet:Going Deeper with Convolutions]]></title>
    <url>%2F2019%2F03%2F19%2Fgooglenet%2F</url>
    <content type="text"><![CDATA[介绍 GoogLeNet模型由Google团队提出，在ILSVRC-2014比赛中获得分类任务第一名。GoogLeNet设计了Inception模块，也即Inception-V1版本。 Inception Inception模块使用不同大小的卷积核提取特征并组合在一起实现不同尺度的特征融合。 其中1x1,3x3,5x5卷积核使用的padding分别为0,1,2,保证了卷积前后feature map大小完全一致，3x3 max pooling步长为1，可以直接在通道维度拼接不同卷积的结果。 为减少计算量，在3x3及5x5卷积核前引入1x1卷积调整通道数目降低维度, 3x3 max pooling后也引入1x1卷积降维， 如下图所示。 GoogleNet GoogLeNet将相同结构的Inception模块堆叠起来，共22层，模型结构如下： 其中输入为224x224x3, 首先仍然使用传统7x7卷积，然后依据feature map大小引入三个层次的Inception模块，分别为inception(3a,3b,4a,4b,4c,4d,4e,5a,5b)，前面的数字表示相对于原始输入特征大小缩小了2的几次方倍，字母a,b等表示级联的顺序，前一个Inception模块融合后的结果将作为下一个Inception模块的输入。不同层次的Inception模块间通过步长为2的max pooling特征图大小减半。最终通过softmax实现多分类任务。 因网络较深，不利于loss反向传递更新参数，在Inception(4a, 4d) 后引入了两个小的网络进行辅助训练。辅助网络同样进行多分类，不参与模型测试。 模型整体流程见下图： 结果 在ILSVRC-2014分类任务上获得冠军，top-5 error 为6.67%。 参考文献 Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9).]]></content>
      <categories>
        <category>computer vision</category>
        <category>classification</category>
      </categories>
      <tags>
        <tag>classification</tag>
        <tag>CVPR2015</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VGG:Very deep convolutional networks for large-scale image recognition]]></title>
    <url>%2F2019%2F03%2F19%2Fvgg%2F</url>
    <content type="text"><![CDATA[介绍 VGG模型来自牛津大学Visual Geometry Group, 其堆叠一系列3x3卷积核，在ILSVRC-2014获得定位任务第一名，分类任务第二名。 架构 模型仅使用3x3及1x1大小的卷积核，3x3为可捕获上下左右信息的最小卷积窗口。所有的3x3卷积有padding=1，步长为1，因此输入输出的特征图大小不变。 池化层窗口大小为2x2,步长为2，每次池化后特征图大小减半。紧接池化后的卷积通道数翻倍，直至最后为512，保持运算的均衡。 两层连续堆叠的3x3卷积与直接5x5卷积有大小相同的感受野，三层连续堆叠的3x3卷积与直接7x7卷积有大小相同的感受野，因此可使用堆叠的较小卷积核来替代较大的卷积。 堆叠的小卷积核相较于单层大卷积核有更强的判别能力。 因堆叠后层数更深，卷积之间有更多的非线性变换(relu), 可增加模型的判别能力。 堆叠的小卷积核相较于单层大卷积核有更少的参数。 如3个3x3通道为C的卷积核参数量为\(3(3^2C^2)=27C^2\), 而1个7x7通道为C的卷积核参数量为\(7^2C^2=49C^2\)。 不同深度的模型配置如下表： 实验 多尺度训练 多模型集成 参考文献 Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.]]></content>
      <categories>
        <category>computer vision</category>
        <category>classification</category>
      </categories>
      <tags>
        <tag>classification</tag>
        <tag>ICLR2015</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[surf]]></title>
    <url>%2F2019%2F03%2F13%2Fsurf%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>features</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>descriptor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hog]]></title>
    <url>%2F2019%2F03%2F13%2Fhog%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>features</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>descriptor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lbp]]></title>
    <url>%2F2019%2F03%2F13%2Flbp%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>features</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>descriptor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dpm]]></title>
    <url>%2F2019%2F03%2F13%2Fdpm%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>features</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>descriptor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sift]]></title>
    <url>%2F2019%2F03%2F13%2Fsift%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>features</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>descriptor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fsr-gan]]></title>
    <url>%2F2019%2F03%2F13%2Ffsr-gan%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>super resolution</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>super resolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[edsr]]></title>
    <url>%2F2019%2F03%2F13%2Fedsr%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>super resolution</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>super resolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[srdensenet]]></title>
    <url>%2F2019%2F03%2F13%2Fsrdensenet%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>super resolution</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>super resolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lapsrn]]></title>
    <url>%2F2019%2F03%2F13%2Flapsrn%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>super resolution</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>super resolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[drrn]]></title>
    <url>%2F2019%2F03%2F13%2Fdrrn%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>super resolution</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>super resolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rdn]]></title>
    <url>%2F2019%2F03%2F13%2Frdn%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>super resolution</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>super resolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[srgan]]></title>
    <url>%2F2019%2F03%2F13%2Fsrgan%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>super resolution</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>super resolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[memnet]]></title>
    <url>%2F2019%2F03%2F13%2Fmemnet%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>super resolution</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>super resolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[stn]]></title>
    <url>%2F2019%2F03%2F13%2Fstn%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>super resolution</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>super resolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[espcn]]></title>
    <url>%2F2019%2F03%2F13%2Fespcn%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>super resolution</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>super resolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[drcn]]></title>
    <url>%2F2019%2F03%2F13%2Fdrcn%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>super resolution</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>super resolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vdsr]]></title>
    <url>%2F2019%2F03%2F13%2Fvdsr%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>super resolution</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>super resolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[srcnn]]></title>
    <url>%2F2019%2F03%2F13%2Fsrcnn%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>super resolution</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>super resolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[panet]]></title>
    <url>%2F2019%2F03%2F13%2Fpanet%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>segmentation</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fcis]]></title>
    <url>%2F2019%2F03%2F13%2Ffcis%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>segmentation</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[instance_fcn]]></title>
    <url>%2F2019%2F03%2F13%2Finstance-fcn%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>segmentation</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mask_x_rcnn]]></title>
    <url>%2F2019%2F03%2F13%2Fmask-x-rcnn%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>segmentation</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mask_rcnn]]></title>
    <url>%2F2019%2F03%2F13%2Fmask-rcnn%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>segmentation</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gcn]]></title>
    <url>%2F2019%2F03%2F13%2Fgcn%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>segmentation</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pspnet]]></title>
    <url>%2F2019%2F03%2F13%2Fpspnet%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>segmentation</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[refinenet]]></title>
    <url>%2F2019%2F03%2F13%2Frefinenet%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>segmentation</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deconvolution]]></title>
    <url>%2F2019%2F03%2F13%2Fdeconvolution%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>segmentation</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[unet]]></title>
    <url>%2F2019%2F03%2F13%2Funet%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>segmentation</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[segnet]]></title>
    <url>%2F2019%2F03%2F13%2Fsegnet%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>segmentation</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mnc]]></title>
    <url>%2F2019%2F03%2F13%2Fmnc%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>segmentation</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deepmask]]></title>
    <url>%2F2019%2F03%2F13%2Fdeepmask%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>segmentation</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fcn]]></title>
    <url>%2F2019%2F03%2F13%2Ffcn%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>segmentation</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deeplab_v3]]></title>
    <url>%2F2019%2F03%2F13%2Fdeeplab-v3%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>segmentation</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sds]]></title>
    <url>%2F2019%2F03%2F13%2Fsds%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>segmentation</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deeplab_v2]]></title>
    <url>%2F2019%2F03%2F13%2Fdeeplab-v2%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>segmentation</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deeplab_v1]]></title>
    <url>%2F2019%2F03%2F13%2Fdeeplab-v1%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>segmentation</category>
      </categories>
      <tags>
        <tag>CVPR2016</tag>
        <tag>segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RetinaNet]]></title>
    <url>%2F2019%2F03%2F13%2Fretinanet%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>detection</category>
      </categories>
      <tags>
        <tag>detection</tag>
        <tag>CVPR2016</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FPN]]></title>
    <url>%2F2019%2F03%2F13%2Ffpn%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>detection</category>
      </categories>
      <tags>
        <tag>detection</tag>
        <tag>CVPR2016</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R-FCN]]></title>
    <url>%2F2019%2F03%2F13%2Fr-fcn%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>detection</category>
      </categories>
      <tags>
        <tag>detection</tag>
        <tag>CVPR2016</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fssd]]></title>
    <url>%2F2019%2F03%2F13%2Ffssd%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>detection</category>
      </categories>
      <tags>
        <tag>detection</tag>
        <tag>CVPR2016</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dssd]]></title>
    <url>%2F2019%2F03%2F13%2Fdssd%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>detection</category>
      </categories>
      <tags>
        <tag>detection</tag>
        <tag>CVPR2016</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yolov3]]></title>
    <url>%2F2019%2F03%2F13%2Fyolov3%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>detection</category>
      </categories>
      <tags>
        <tag>detection</tag>
        <tag>CVPR2016</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yolo9000]]></title>
    <url>%2F2019%2F03%2F13%2Fyolo9000%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>detection</category>
      </categories>
      <tags>
        <tag>detection</tag>
        <tag>CVPR2016</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yolo]]></title>
    <url>%2F2019%2F03%2F13%2Fyolo%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>detection</category>
      </categories>
      <tags>
        <tag>detection</tag>
        <tag>CVPR2016</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssd]]></title>
    <url>%2F2019%2F03%2F13%2Fssd%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>detection</category>
      </categories>
      <tags>
        <tag>detection</tag>
        <tag>CVPR2016</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[edge-boxes]]></title>
    <url>%2F2019%2F03%2F13%2Fedge-boxes%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>detection</category>
      </categories>
      <tags>
        <tag>detection</tag>
        <tag>CVPR2016</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[selective-search]]></title>
    <url>%2F2019%2F03%2F13%2Fselective-search%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>detection</category>
      </categories>
      <tags>
        <tag>detection</tag>
        <tag>CVPR2016</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Faster-RCNN]]></title>
    <url>%2F2019%2F03%2F13%2Ffaster-rcnn%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>detection</category>
      </categories>
      <tags>
        <tag>detection</tag>
        <tag>CVPR2016</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fast-RCNN]]></title>
    <url>%2F2019%2F03%2F13%2Ffast-rcnn%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>detection</category>
      </categories>
      <tags>
        <tag>detection</tag>
        <tag>CVPR2016</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sppnet]]></title>
    <url>%2F2019%2F03%2F13%2Fsppnet%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>detection</category>
      </categories>
      <tags>
        <tag>detection</tag>
        <tag>CVPR2016</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rcnn]]></title>
    <url>%2F2019%2F03%2F13%2Frcnn%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>computer vision</category>
        <category>detection</category>
      </categories>
      <tags>
        <tag>detection</tag>
        <tag>CVPR2016</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AlexNet:ImageNet Classification with Deep Convolutional Neural Networks]]></title>
    <url>%2F2019%2F03%2F12%2Falexnet%2F</url>
    <content type="text"><![CDATA[介绍 AlexNet根据其作者Alex命名,Alex是深度学习之父Hinton的学生。AlexNet赢得了ILSVRC-2012比赛的冠军，点燃了深度学习之火。 数据集 ImageNet是由斯坦福李飞飞主导发布大规模图像数据集, 包含超过1500万张有标注信息的高清图片，从属于约22000个类别。ILSVRC使用ImageNet的一个子集，共有1000个目标类别，其中约有1200万张用作训练集，5万张用于验证集，15万张用于测试集。 分类任务使用top-1 error和top-5 error两个评价指标。top-1 error指在测试集上预测错误的图片所占的比率，top-5 error指的是在测试集上预测概率最高的五个类别均不包含正确类别的比率。 ImageNet包含大小不同的图片，模型需要固定大小的输入。文章首先把原始图片缩放至短边大小为256，再从中截取256x256的patch。 架构 AlexNet包含8个可学习参数的层，其中5个卷积层，3个全连接层。摘自torchvision仓库的实现如下： 123456789101112131415161718192021222324252627282930313233343536class AlexNet(nn.Module): def __init__(self, num_classes=1000): super(AlexNet, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(64, 192, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(192, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.avgpool = nn.AdaptiveAvgPool2d((6, 6)) self.classifier = nn.Sequential( nn.Dropout(), nn.Linear(256 * 6 * 6, 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Linear(4096, num_classes), ) def forward(self, x): x = self.features(x) x = self.avgpool(x) x = x.view(x.size(0), 256 * 6 * 6) x = self.classifier(x) return x 文章认为对分类结果有帮助的几个点： 使用Relu f(x) = max(0, x) 多GPU训练 局部响应正则化 后被其它文章证实无用，并且会带来额外计算负担。 重叠池化 实验 数据增广 水平镜像及在256x256图片上密集取224x224 patch。 随机改变RGB通道上亮度值。 Dropout 结果 在ILSVRC—2010上获得top-1 error为37.5%，top-5 error为17.0%。ILSVRC-2012上top-5 error为15.3%，第二名为26.2%。 参考文献 Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 1097-1105.]]></content>
      <categories>
        <category>computer vision</category>
        <category>classification</category>
      </categories>
      <tags>
        <tag>classification</tag>
        <tag>NIPS2012</tag>
      </tags>
  </entry>
</search>
