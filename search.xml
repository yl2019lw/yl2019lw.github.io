<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Batch Normalization:Accelerating Deep Neural Network Training by Reducing Internal Covariate Shift</title>
      <link href="/2019/03/21/batchnorm/"/>
      <url>/2019/03/21/batchnorm/</url>
      
        <content type="html"><![CDATA[<h1 id="介绍">介绍</h1><p>文章提出Batch Normalization，解决了网络中每层处理单元的输入分布随参数更新变化的问题，也即internal covariate shift，使得能够使用较大的学习率，不用过多关注初始化，减少了使用dropout的必要，能够加快网络的训练和获得更好的性能。</p><p>在训练神经网络时，浅层参数的更新会影响后续层的输入，导致后续层需要适应其输入分布的变化。对于sigmoid这类饱和激活函数而言，当其输入数据落入其饱和区时会导致梯度接近于0，参数更新变小，影响网络收敛速度。</p><p>通常我们使用白化操作对输入数据进行预处理，主要是PCA白化和ZCA白化，通过白化操作会去除特征之间的相关性，使得输入分布有相同的均值和方差，但对网络中的每一层数据都进行白化操作成本太高，因此提出了batch normalization，使得网络中每一层处理单元的输入有相同的分布。</p><h1 id="batch-normalization">Batch Normalization</h1><p>Batch normalization对同一batch内网络中的每一层的每一个特征，首先进行归一化至均值为0，标准差为1，对于d维输入$x = (x^{(1)}, x^{(2)}, ..., x^{(d)}) $ 有：</p><center><span class="math inline">\({\hat{x}}^{(k)} = \frac{ x^{(k)} - E[x^{(k)}] }{\sqrt{Var[x^{(k)}]}}\)</span></center><p>然后还原数据的表达能力：</p><center><span class="math inline">\({\hat{y}}^{(k)} = \gamma^{(k)} \hat{x}^{(k)} + \beta^{(k)}\)</span></center><p>其中<span class="math inline">\(\gamma^{(k)},\beta^{(k)}\)</span>是第k处需要学习的一对参数。经过上述处理后，对应k处数据有稳定的均值为<span class="math inline">\(\beta^{(k)}\)</span>，标准差为<span class="math inline">\(\gamma^{(k)}\)</span>的分布。 最终将网络中每一层中原始的数据<span class="math inline">\(x^{(k)}\)</span>替换为<span class="math inline">\(y^{(k)}\)</span>。算法描述见下图：</p><img src="/2019/03/21/batchnorm/batch_norm_single.png" width="600" title="batch norm single"><p>对于全连接层，针对每个激活的神经元进行处理。对于卷积层，针对每一个feature map学习一对<span class="math inline">\(\beta^{(k)}\)</span>，<span class="math inline">\(\gamma^{(k)}\)</span>。</p><p>在测试时，网络中每一层的输入分布已经固定，其均值与标准差通过moving average的方式由训练时求得的<span class="math inline">\(\beta^{(k)}\)</span>，<span class="math inline">\(\gamma^{(k)}\)</span>计算而来，整体流程如下：</p><img src="/2019/03/21/batchnorm/batch_norm_all.png" width="600" title="batch norm all"><h1 id="总结">总结</h1><p>Batch Normalization加快了训练速度，能够获取更好的性能。BN-Inception是增加了batch normaliztion的inception模型，也即使Inception-V2，在ImageNet上获得了top-5 error为4.82%。</p><h1 id="参考文献">参考文献</h1><blockquote>Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167.</blockquote>]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> classification </category>
          
      </categories>
      
      
        <tags>
            
            <tag> classification </tag>
            
            <tag> ICML2015 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Resnet:Deep Residual Learning for Image Recognition</title>
      <link href="/2019/03/19/resnet/"/>
      <url>/2019/03/19/resnet/</url>
      
        <content type="html"><![CDATA[<h1 id="介绍">介绍</h1><p>文章提出残差网络使得能够更容易地训练出更深层次的神经网络，Resnet模型给出了高至152层的网络，但却比VGG有更小的复杂度。Resnet模型横扫了ILSVRC-2015，COCO-2015上的分类、检测、分割等比赛任务的冠军。</p><p>更深的神经网络可能带来更好的性能，但却存在难以训练的退化问题，这并不是因为更深的网络导致了过拟合，而是训练误差都不能得到很好地下降，如下图所示：</p><img src="/2019/03/19/resnet/degradation.png" width="600" title="degradation"><p>退化问题说明了深度模型训练的困难。从理论分析来看，考虑一个浅层网络和在其之上加了更多层的深层网络，对于深层网络可以把额外增加的层训练为恒等映射，其余部分从浅层网络拷贝过来，那么对于深层网络就应该存在方案使得其训练误差可以比相应的浅层网络要小。实际训练网络的情况并非如此，本文使用残差学习解决这个问题。</p><h1 id="deep-residual-learning">Deep Residual Learning</h1><h2 id="residual-learning">Residual Learning</h2><p>假设<span class="math inline">\(H(x)\)</span>是要通过堆叠的网络层学习的从输入到输出的映射，<span class="math inline">\(x\)</span>表示输入，如果假定多层网络能够渐近地学习到<span class="math inline">\(H(x)\)</span>，那么其也应该能学习到<span class="math inline">\(F(x)=H(x)-x\)</span>。通过残差方式只需要学习<span class="math inline">\(F(x)\)</span>，原始问题为<span class="math inline">\(F(x)+x\)</span>，两种方式的学习难易程度却有很大的不同。</p><img src="/2019/03/19/resnet/residual.png" width="400" title="degradation"><h2 id="identity-mapping-by-shortcuts">Identity Mapping by Shortcuts</h2><p>残差学习可形式化地表达为<span class="math inline">\(y=F(x,{W_i})+x\)</span>，其中<span class="math inline">\(x,y\)</span>是当前整个块的输入和输出，<span class="math inline">\(F(x,{W_i})\)</span>表示需要学习的残差映射。如上图所示为一个残差building block，包含两层，则<span class="math inline">\(F=W_2\sigma(W_1x)\)</span>，其中<span class="math inline">\(\sigma\)</span>表示Relu函数。由公式可以看出，残差方式没有带来额外的参数，也基本没有带来更多地运算，因此可以公平地与相应地plain network进行比较。当<span class="math inline">\(x\)</span>与<span class="math inline">\(F\)</span>有相同的维度时，直接进行element-wise相加即可，当维度不同时可对<span class="math inline">\(x\)</span>投影到匹配的维度后再进行相加，此时有<span class="math inline">\(y=F(x,{W_i})+W_sx\)</span>。当维度本就相同时也可使用方阵形式的<span class="math inline">\(W_s\)</span>。</p><h2 id="network-architecture">Network Architecture</h2><ul><li><p>plain network</p><p>plain network如同VGG一样堆叠3x3卷积网络，遵循两个设计原则：</p><ul><li>对于输出的feature map大小一样的层有相同数量的卷积核。</li><li>如果输出的feature map大小减半，则卷积核的数量翻倍以保持每层的复杂度。</li></ul></li><li><p>residual network</p><p>residual network在前述的plain network上插入shortcut connection。当输入与输出维度相同时，可直接使用identity shortcut。当维度不同时，可仍选择shortcut mapping但进行额外补0以使维度匹配，或者使用projection shortcut。当shortcut connection跨越不同大小的feature map时，projection使用步长为2。</p></li><li><p>bottleneck</p><p>当配置的网络较深时存在参数过多的问题，因此可引入1x1的卷积核来降低维度减少计算量。具体来说，2层的3x3残差块中可由1x1,3x3,1x1堆叠的三层块代替，前面的1x1卷积用来降低维度，后面的1x1用来提升维度以使输入输出维度匹配，如下图所示：</p></li></ul><img src="/2019/03/19/resnet/bottleneck.png" width="400" title="bottleneck"><ul><li><p>architecture</p><p>常用resnet有18，34，50，101及152层，其中50层及以上使用bottleneck版本的块。resnet首先使用步长为2的7x7卷积，然后包含四组不同feature map大小的堆叠的残差块，因此最终feature map的大小为输入的1/32。resnet中无全连接层，在最终的feature map上执行全局池化后输入softmax进行分类。不同深度的resnet配置如下表：</p></li></ul><img src="/2019/03/19/resnet/resnet.png" title="resnet"><h1 id="参考文献">参考文献</h1><blockquote>He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).</blockquote>]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> classification </category>
          
      </categories>
      
      
        <tags>
            
            <tag> classification </tag>
            
            <tag> CVPR2016 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GoogLeNet:Going Deeper with Convolutions</title>
      <link href="/2019/03/19/googlenet/"/>
      <url>/2019/03/19/googlenet/</url>
      
        <content type="html"><![CDATA[<h1 id="介绍">介绍</h1><p>GoogLeNet模型由Google团队提出，在ILSVRC-2014比赛中获得分类任务第一名。GoogLeNet设计了Inception模块，也即Inception-V1版本。</p><h1 id="inception">Inception</h1><p>Inception模块使用不同大小的卷积核提取特征并组合在一起实现不同尺度的特征融合。 其中1x1,3x3,5x5卷积核使用的padding分别为0,1,2,保证了卷积前后feature map大小完全一致，3x3 max pooling步长为1，可以直接在通道维度拼接不同卷积的结果。</p><p>为减少计算量，在3x3及5x5卷积核前引入1x1卷积调整通道数目降低维度, 3x3 max pooling后也引入1x1卷积降维， 如下图所示。</p><img src="/2019/03/19/googlenet/inception.png" width="400" title="inception"><h1 id="googlenet">GoogleNet</h1><p>GoogLeNet将相同结构的Inception模块堆叠起来，共22层，模型结构如下：</p><p>其中输入为224x224x3, 首先仍然使用传统7x7卷积，然后依据feature map大小引入三个层次的Inception模块，分别为inception(3a,3b,4a,4b,4c,4d,4e,5a,5b)，前面的数字表示相对于原始输入特征大小缩小了2的几次方倍，字母a,b等表示级联的顺序，前一个Inception模块融合后的结果将作为下一个Inception模块的输入。不同层次的Inception模块间通过步长为2的max pooling特征图大小减半。最终通过softmax实现多分类任务。</p><img src="/2019/03/19/googlenet/googlenet-arch.png" title="This is googlenet architecture"><p>因网络较深，不利于loss反向传递更新参数，在Inception(4a, 4d) 后引入了两个小的网络进行辅助训练。辅助网络同样进行多分类，不参与模型测试。 模型整体流程见下图： <img src="/2019/03/19/googlenet/googlenet.jpg" title="This is googlenet architecture"></p><h1 id="结果">结果</h1><p>在ILSVRC-2014分类任务上获得冠军，top-5 error 为6.67%。</p><h1 id="参考文献">参考文献</h1><blockquote>Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9).</blockquote>]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> classification </category>
          
      </categories>
      
      
        <tags>
            
            <tag> classification </tag>
            
            <tag> CVPR2015 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VGG:Very deep convolutional networks for large-scale image recognition</title>
      <link href="/2019/03/19/vgg/"/>
      <url>/2019/03/19/vgg/</url>
      
        <content type="html"><![CDATA[<h1 id="介绍">介绍</h1><p>VGG模型来自牛津大学Visual Geometry Group, 其堆叠一系列3x3卷积核，在ILSVRC-2014获得定位任务第一名，分类任务第二名。</p><h1 id="架构">架构</h1><p>模型仅使用3x3及1x1大小的卷积核，3x3为可捕获上下左右信息的最小卷积窗口。所有的3x3卷积有padding=1，步长为1，因此输入输出的特征图大小不变。</p><p>池化层窗口大小为2x2,步长为2，每次池化后特征图大小减半。紧接池化后的卷积通道数翻倍，直至最后为512，保持运算的均衡。</p><p>两层连续堆叠的3x3卷积与直接5x5卷积有大小相同的感受野，三层连续堆叠的3x3卷积与直接7x7卷积有大小相同的感受野，因此可使用堆叠的较小卷积核来替代较大的卷积。</p><ul><li><p>堆叠的小卷积核相较于单层大卷积核有更强的判别能力。</p><p>因堆叠后层数更深，卷积之间有更多的非线性变换(relu), 可增加模型的判别能力。</p></li><li><p>堆叠的小卷积核相较于单层大卷积核有更少的参数。</p><p>如3个3x3通道为C的卷积核参数量为<span class="math inline">\(3(3^2C^2)=27C^2\)</span>, 而1个7x7通道为C的卷积核参数量为<span class="math inline">\(7^2C^2=49C^2\)</span>。</p></li></ul><p>不同深度的模型配置如下表：</p><img src="/2019/03/19/vgg/vgg.png" title="This is VGG architecture"><h1 id="实验">实验</h1><ul><li><p>多尺度训练</p></li><li><p>多模型集成</p></li></ul><h1 id="参考文献">参考文献</h1><blockquote>Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.</blockquote>]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> classification </category>
          
      </categories>
      
      
        <tags>
            
            <tag> classification </tag>
            
            <tag> ICLR2015 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>surf</title>
      <link href="/2019/03/13/surf/"/>
      <url>/2019/03/13/surf/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> features </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> descriptor </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hog</title>
      <link href="/2019/03/13/hog/"/>
      <url>/2019/03/13/hog/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> features </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> descriptor </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>lbp</title>
      <link href="/2019/03/13/lbp/"/>
      <url>/2019/03/13/lbp/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> features </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> descriptor </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>dpm</title>
      <link href="/2019/03/13/dpm/"/>
      <url>/2019/03/13/dpm/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> features </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> descriptor </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sift</title>
      <link href="/2019/03/13/sift/"/>
      <url>/2019/03/13/sift/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> features </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> descriptor </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fsr-gan</title>
      <link href="/2019/03/13/fsr-gan/"/>
      <url>/2019/03/13/fsr-gan/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> super resolution </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> super resolution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>edsr</title>
      <link href="/2019/03/13/edsr/"/>
      <url>/2019/03/13/edsr/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> super resolution </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> super resolution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>srdensenet</title>
      <link href="/2019/03/13/srdensenet/"/>
      <url>/2019/03/13/srdensenet/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> super resolution </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> super resolution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>lapsrn</title>
      <link href="/2019/03/13/lapsrn/"/>
      <url>/2019/03/13/lapsrn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> super resolution </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> super resolution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>drrn</title>
      <link href="/2019/03/13/drrn/"/>
      <url>/2019/03/13/drrn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> super resolution </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> super resolution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rdn</title>
      <link href="/2019/03/13/rdn/"/>
      <url>/2019/03/13/rdn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> super resolution </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> super resolution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>srgan</title>
      <link href="/2019/03/13/srgan/"/>
      <url>/2019/03/13/srgan/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> super resolution </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> super resolution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>memnet</title>
      <link href="/2019/03/13/memnet/"/>
      <url>/2019/03/13/memnet/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> super resolution </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> super resolution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>stn</title>
      <link href="/2019/03/13/stn/"/>
      <url>/2019/03/13/stn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> super resolution </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> super resolution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>espcn</title>
      <link href="/2019/03/13/espcn/"/>
      <url>/2019/03/13/espcn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> super resolution </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> super resolution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>drcn</title>
      <link href="/2019/03/13/drcn/"/>
      <url>/2019/03/13/drcn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> super resolution </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> super resolution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vdsr</title>
      <link href="/2019/03/13/vdsr/"/>
      <url>/2019/03/13/vdsr/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> super resolution </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> super resolution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>srcnn</title>
      <link href="/2019/03/13/srcnn/"/>
      <url>/2019/03/13/srcnn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> super resolution </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> super resolution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>panet</title>
      <link href="/2019/03/13/panet/"/>
      <url>/2019/03/13/panet/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fcis</title>
      <link href="/2019/03/13/fcis/"/>
      <url>/2019/03/13/fcis/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>instance_fcn</title>
      <link href="/2019/03/13/instance-fcn/"/>
      <url>/2019/03/13/instance-fcn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mask_x_rcnn</title>
      <link href="/2019/03/13/mask-x-rcnn/"/>
      <url>/2019/03/13/mask-x-rcnn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mask_rcnn</title>
      <link href="/2019/03/13/mask-rcnn/"/>
      <url>/2019/03/13/mask-rcnn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>gcn</title>
      <link href="/2019/03/13/gcn/"/>
      <url>/2019/03/13/gcn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pspnet</title>
      <link href="/2019/03/13/pspnet/"/>
      <url>/2019/03/13/pspnet/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>refinenet</title>
      <link href="/2019/03/13/refinenet/"/>
      <url>/2019/03/13/refinenet/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>deconvolution</title>
      <link href="/2019/03/13/deconvolution/"/>
      <url>/2019/03/13/deconvolution/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>unet</title>
      <link href="/2019/03/13/unet/"/>
      <url>/2019/03/13/unet/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>segnet</title>
      <link href="/2019/03/13/segnet/"/>
      <url>/2019/03/13/segnet/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mnc</title>
      <link href="/2019/03/13/mnc/"/>
      <url>/2019/03/13/mnc/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>deepmask</title>
      <link href="/2019/03/13/deepmask/"/>
      <url>/2019/03/13/deepmask/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fcn</title>
      <link href="/2019/03/13/fcn/"/>
      <url>/2019/03/13/fcn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>deeplab_v3</title>
      <link href="/2019/03/13/deeplab-v3/"/>
      <url>/2019/03/13/deeplab-v3/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sds</title>
      <link href="/2019/03/13/sds/"/>
      <url>/2019/03/13/sds/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>deeplab_v2</title>
      <link href="/2019/03/13/deeplab-v2/"/>
      <url>/2019/03/13/deeplab-v2/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>deeplab_v1</title>
      <link href="/2019/03/13/deeplab-v1/"/>
      <url>/2019/03/13/deeplab-v1/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> segmentation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR2016 </tag>
            
            <tag> segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RetinaNet</title>
      <link href="/2019/03/13/retinanet/"/>
      <url>/2019/03/13/retinanet/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> CVPR2016 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FPN</title>
      <link href="/2019/03/13/fpn/"/>
      <url>/2019/03/13/fpn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> CVPR2016 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>R-FCN</title>
      <link href="/2019/03/13/r-fcn/"/>
      <url>/2019/03/13/r-fcn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> CVPR2016 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fssd</title>
      <link href="/2019/03/13/fssd/"/>
      <url>/2019/03/13/fssd/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> CVPR2016 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>dssd</title>
      <link href="/2019/03/13/dssd/"/>
      <url>/2019/03/13/dssd/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> CVPR2016 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>yolov3</title>
      <link href="/2019/03/13/yolov3/"/>
      <url>/2019/03/13/yolov3/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> CVPR2016 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>yolo9000</title>
      <link href="/2019/03/13/yolo9000/"/>
      <url>/2019/03/13/yolo9000/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> CVPR2016 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>yolo</title>
      <link href="/2019/03/13/yolo/"/>
      <url>/2019/03/13/yolo/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> CVPR2016 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ssd</title>
      <link href="/2019/03/13/ssd/"/>
      <url>/2019/03/13/ssd/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> CVPR2016 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>edge-boxes</title>
      <link href="/2019/03/13/edge-boxes/"/>
      <url>/2019/03/13/edge-boxes/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> CVPR2016 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>selective-search</title>
      <link href="/2019/03/13/selective-search/"/>
      <url>/2019/03/13/selective-search/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> CVPR2016 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Faster-RCNN</title>
      <link href="/2019/03/13/faster-rcnn/"/>
      <url>/2019/03/13/faster-rcnn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> CVPR2016 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Fast-RCNN</title>
      <link href="/2019/03/13/fast-rcnn/"/>
      <url>/2019/03/13/fast-rcnn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> CVPR2016 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sppnet</title>
      <link href="/2019/03/13/sppnet/"/>
      <url>/2019/03/13/sppnet/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> CVPR2016 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rcnn</title>
      <link href="/2019/03/13/rcnn/"/>
      <url>/2019/03/13/rcnn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detection </tag>
            
            <tag> CVPR2016 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>identity-mapping</title>
      <link href="/2019/03/12/identity-mapping/"/>
      <url>/2019/03/12/identity-mapping/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> classification </category>
          
      </categories>
      
      
        <tags>
            
            <tag> classification </tag>
            
            <tag> NIPS2012 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>stochastic_depth</title>
      <link href="/2019/03/12/stochastic-depth/"/>
      <url>/2019/03/12/stochastic-depth/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> classification </category>
          
      </categories>
      
      
        <tags>
            
            <tag> classification </tag>
            
            <tag> NIPS2012 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>resnext</title>
      <link href="/2019/03/12/resnext/"/>
      <url>/2019/03/12/resnext/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> classification </category>
          
      </categories>
      
      
        <tags>
            
            <tag> classification </tag>
            
            <tag> NIPS2012 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>inception-v4</title>
      <link href="/2019/03/12/inception-v4/"/>
      <url>/2019/03/12/inception-v4/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> classification </category>
          
      </categories>
      
      
        <tags>
            
            <tag> classification </tag>
            
            <tag> NIPS2012 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>inception_v3</title>
      <link href="/2019/03/12/inception-v3/"/>
      <url>/2019/03/12/inception-v3/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> classification </category>
          
      </categories>
      
      
        <tags>
            
            <tag> classification </tag>
            
            <tag> NIPS2012 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>wrn</title>
      <link href="/2019/03/12/wrn/"/>
      <url>/2019/03/12/wrn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> classification </category>
          
      </categories>
      
      
        <tags>
            
            <tag> classification </tag>
            
            <tag> ICLR2015 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>densenet</title>
      <link href="/2019/03/12/densenet/"/>
      <url>/2019/03/12/densenet/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> classification </category>
          
      </categories>
      
      
        <tags>
            
            <tag> classification </tag>
            
            <tag> NIPS2012 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>senet</title>
      <link href="/2019/03/12/senet/"/>
      <url>/2019/03/12/senet/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> classification </category>
          
      </categories>
      
      
        <tags>
            
            <tag> classification </tag>
            
            <tag> NIPS2012 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AlexNet:ImageNet Classification with Deep Convolutional Neural Networks</title>
      <link href="/2019/03/12/alexnet/"/>
      <url>/2019/03/12/alexnet/</url>
      
        <content type="html"><![CDATA[<h1 id="介绍">介绍</h1><p>AlexNet根据其作者Alex命名,Alex是深度学习之父Hinton的学生。AlexNet赢得了ILSVRC-2012比赛的冠军，点燃了深度学习之火。</p><h1 id="数据集">数据集</h1><p>ImageNet是由斯坦福李飞飞主导发布大规模图像数据集, 包含超过1500万张有标注信息的高清图片，从属于约22000个类别。ILSVRC使用ImageNet的一个子集，共有1000个目标类别，其中约有1200万张用作训练集，5万张用于验证集，15万张用于测试集。</p><p>分类任务使用top-1 error和top-5 error两个评价指标。top-1 error指在测试集上预测错误的图片所占的比率，top-5 error指的是在测试集上预测概率最高的五个类别均不包含正确类别的比率。</p><p>ImageNet包含大小不同的图片，模型需要固定大小的输入。文章首先把原始图片缩放至短边大小为256，再从中截取256x256的patch。</p><h1 id="架构">架构</h1><p>AlexNet包含8个可学习参数的层，其中5个卷积层，3个全连接层。摘自torchvision仓库的实现如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">class AlexNet(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, num_classes=1000):</span><br><span class="line">        super(AlexNet, self).__init__()</span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            nn.MaxPool2d(kernel_size=3, stride=2),</span><br><span class="line">            nn.Conv2d(64, 192, kernel_size=5, padding=2),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            nn.MaxPool2d(kernel_size=3, stride=2),</span><br><span class="line">            nn.Conv2d(192, 384, kernel_size=3, padding=1),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            nn.Conv2d(384, 256, kernel_size=3, padding=1),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            nn.Conv2d(256, 256, kernel_size=3, padding=1),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            nn.MaxPool2d(kernel_size=3, stride=2),</span><br><span class="line">        )</span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Dropout(),</span><br><span class="line">            nn.Linear(256 * 6 * 6, 4096),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            nn.Dropout(),</span><br><span class="line">            nn.Linear(4096, 4096),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            nn.Linear(4096, num_classes),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = x.view(x.size(0), 256 * 6 * 6)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure><p>文章认为对分类结果有帮助的几个点：</p><ul><li><p>使用Relu</p><pre><code>  f(x) = max(0, x)</code></pre></li><li><p>多GPU训练</p></li><li><p>局部响应正则化</p><pre><code>  后被其它文章证实无用，并且会带来额外计算负担。</code></pre></li><li><p>重叠池化</p></li></ul><h1 id="实验">实验</h1><ul><li><p>数据增广</p><ul><li>水平镜像及在256x256图片上密集取224x224 patch。</li><li>随机改变RGB通道上亮度值。</li></ul></li><li><p>Dropout</p></li><li><p>结果</p><pre><code>  在ILSVRC—2010上获得top-1 error为37.5%，top-5 error为17.0%。ILSVRC-2012上top-5 error为15.3%，第二名为26.2%。</code></pre></li></ul><h1 id="参考文献">参考文献</h1><blockquote>Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 1097-1105.</blockquote>]]></content>
      
      
      <categories>
          
          <category> computer vision </category>
          
          <category> classification </category>
          
      </categories>
      
      
        <tags>
            
            <tag> classification </tag>
            
            <tag> NIPS2012 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
