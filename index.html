<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.0.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.0.1">


  <link rel="mask-icon" href="/images/logo.svg?v=7.0.1" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.0.1',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="爱学习">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="爱学习">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="爱学习">






  <link rel="canonical" href="http://yoursite.com/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>爱学习</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">爱学习</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/24/senet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yilang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="爱学习">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/24/senet/" class="post-title-link" itemprop="url">SENet:Squeeze-and-Excitation Networks</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-24 12:00:02 / 修改时间：13:46:08" itemprop="dateCreated datePublished" datetime="2019-03-24T12:00:02+08:00">2019-03-24</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/computer-vision/" itemprop="url" rel="index"><span itemprop="name">computer vision</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/computer-vision/classification/" itemprop="url" rel="index"><span itemprop="name">classification</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="介绍">介绍</h1>
<p>文章聚焦在channel之间特征相关性来提升深度网络的表达能力，提出了Squeeze-and-Excitation block来对不同channel的特征进行重标定。文章使用SENet结构获得了ILSVRC-2017分类任务的冠军，在ImageNet数据集上top-5 error低至2.251%。</p>
<p>基本的SE block如下所示：</p>
<img src="/2019/03/24/senet/seblock.png" title="squeeze excitation block">
<p>对于网络中的变换<span class="math inline">\(F_{tr}: X \to U, X \in \mathbb{R}^{H&#39; \times W&#39; \times C&#39;}, U \in \mathbb{R}^{H \times W \times C}\)</span>，我们可以构造SE block来进行特征标定。首先把特征<span class="math inline">\(U\)</span>通过一个squeeze操作，即把feature map在<span class="math inline">\(H \times W\)</span>上进行聚合得到一个channel descriptor。此channel descriptor包含了channel-wise特征响应的全局分布，能够使得全局感受野的信息被网络的低层所利用。然后执行excitation操作，对每个channel通过self-gating机制生成权重，最终使用不同channel的权重对原始U进行处理，把得到的结果替代U作为后续层的输入。</p>
<p>SENet可由许多SE Block堆叠而成，SE Block可以在原始网络的任意深度处作就地替换。在浅层中SE Block更多了excite类别无关信息，在深层中逐渐特化。</p>
<h1 id="squeeze-and-excitation-blocks">Squeeze-and-Excitation Blocks</h1>
<p>对于网络中的变换<span class="math inline">\(F_{tr}: X \to U, X \in \mathbb{R}^{H&#39; \times W&#39; \times C&#39;}, U \in \mathbb{R}^{H \times W \times C}\)</span>，我们简化<span class="math inline">\(F_{tr}\)</span>为一系列卷积操作。命<span class="math inline">\(V = [v_1, v_2, ...v_C]\)</span>表示对应的卷积核，<span class="math inline">\(F_{tr}\)</span>的输出<span class="math inline">\(U = [u_1, u_2, ...u_C]\)</span>，其中 <span class="math display">\[\begin{equation}
    u_c = v_c * X = \sum_{s=1}^{C&#39;} v_c^s * x^s
\end{equation}\]</span> 其中*表示卷积操作，<span class="math inline">\(v_c = [v_c^1, v_c^2, ..., v_c^{C&#39;}], X = [x^1, x^2,...,x^{C&#39;}]\)</span>。由上式可见输出是由输入的每一个channel处都参与summation得到,因此已隐含有channel依赖信息，但此信息与空间位置混在一起。文章的目的在于提升网络对于有用特征的灵敏度，同时抑制无用信息，通过squeeze和excitation操作来显式地建模不同通道特征之间相关性，进而进行特征标定。</p>
<ul>
<li><p>Squeeze: Global Information Embedding</p>
<p>由于原卷积操作只作用于局部感受野，不能利用全局上下文信息，因此首先使用squeeze操作来将全局位置信息编码入channel descriptor。这是通过全局平均池化的方式来得到<span class="math inline">\(z \in \mathbb{R}^C\)</span>，其中第c个元素为： <span class="math display">\[\begin{equation}
  z_c = F_{sq}(u_c) = \frac{1}{H \times W} \sum_{i=1}{H}\sum{j=1}{W}u_c(i, j)
\end{equation}\]</span></p></li>
<li><p>Excitation: Adaptive Recalibration</p>
<p>然后使用excitation捕获channel-wise依赖： <span class="math display">\[\begin{equation}
  s = F_{ex}(z, W) = \sigma(g(z, W)) = \sigma(W_2\sigma(W_1z))
\end{equation}\]</span> 其中<span class="math inline">\(\sigma\)</span>表示Relu函数，<span class="math inline">\(W_1 \in \mathbb{R}^{\frac{C}{r} \times C}, W_2 \in \mathbb{R}^{C \times \frac{C}{r}}\)</span>,即通过两个全连接层来学习非线性权重，其中一个以比率<span class="math inline">\(r\)</span>降低维度，另一个进行还原。最终标定后输出为： <span class="math display">\[\begin{equation}
  \widetilde{x}_c = F_{scale}(u_c, s_c) = s_c \cdot u_c
\end{equation}\]</span> 其中<span class="math inline">\(\widetilde{X} = [\widetilde{x}_1, \widetilde{x}_2, ..., \widetilde{x}_C]\)</span>, <span class="math inline">\(F_{scale}(u_c, s_c)\)</span>为在特征图<span class="math inline">\(u_c \in \mathbb{R}^{H \times W}\)</span>与标量<span class="math inline">\(s_c\)</span>之间channel-wise乘法。</p></li>
</ul>
<p>SENet可由SE Block对原网络结构改造而成，如ResNet残差单元及Inception模块修改后基本结构如下：</p>
<img src="/2019/03/24/senet/schema.png" title="schema 400">
<h1 id="senet">SENet</h1>
<p>SE-ResNet-50及SE-ResNext-50整体组成单元见下表：</p>
<img src="/2019/03/24/senet/se-resnet.png" title="se resnet">
<p>可见仅在原残差块内增加2个全连接层,额外引入参数数量为： <span class="math display">\[\begin{equation}
    \frac{2}{r}\sum_{s=1}^{S}N_s \cdot C_s^2
\end{equation}\]</span> 其中r为reduction ratio，<span class="math inline">\(S\)</span>为不同feature map大小的阶段数目，<span class="math inline">\(C_s\)</span>为第<span class="math inline">\(s\)</span>个阶段的输入特征通道维度，<span class="math inline">\(N_s\)</span>为每个阶段的重复残差块数量。</p>
<p>在ImageNet数据集上运行结果如下：</p>
<img src="/2019/03/24/senet/imagenet.png" title="imagenet">
<h1 id="参考文献">参考文献</h1>
<blockquote>
Hu, J., Shen, L., & Sun, G. (2018). Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 7132-7141).
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/23/stochastic-depth/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yilang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="爱学习">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/23/stochastic-depth/" class="post-title-link" itemprop="url">Deep Networks with Stochastic Depth</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-23 15:39:22 / 修改时间：17:43:24" itemprop="dateCreated datePublished" datetime="2019-03-23T15:39:22+08:00">2019-03-23</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/computer-vision/" itemprop="url" rel="index"><span itemprop="name">computer vision</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/computer-vision/classification/" itemprop="url" rel="index"><span itemprop="name">classification</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="介绍">介绍</h1>
<p>较浅的神经网络表示能力不如较深的网络，但较深的网络相对难以训练，文章提出了stochastic depth方式达到了在训练时使用较浅的网络及在测试时使用较深的网络的效果，最终能大幅减少训练时间并获得性能上的提升。</p>
<p>测试时错误率的降低可归功于两点：一是缩短了期望的训练网络深度使得前向传播和梯度回传更加便捷，二是此种方式训练可以看作很多不同深度的网络模型进行了集成，可以有效地防止过拟合。</p>
<h1 id="stochastic-depth">Stochastic Depth</h1>
<ul>
<li><p>ResNet architecture</p>
<p>ResNet基本残差单元如下： <span class="math display">\[\begin{equation}
  H_l = ReLU(f_l(H_{l-1}) + id(H_{l-1}))
\end{equation}\]</span> 其中<span class="math inline">\(H_l\)</span>是第l层的输出，<span class="math inline">\(f_l\)</span>是由Conv, Batch Norm堆叠的层，如下图所示：</p></li>
</ul>
<img src="/2019/03/23/stochastic-depth/resnet.png" title="resnet">
<ul>
<li><p>Stochastic depth</p>
<p>文章在使用resnet block网络结构基础上，使用stochastic depth减少训练时网络的实际深度，具体而言就是通过随机地以一定概率直接丢弃整块残差单元，然后把相邻块的信息直接相连，此时有： <span class="math display">\[\begin{equation}
  H_l = ReLU(b_lf_l(H_{l_1}) + id(H_{l-1}))
\end{equation}\]</span> 其中<span class="math inline">\(b_l\)</span>为伯努利分布随机变量，<span class="math inline">\(p_l = Pr(b_l=1)\)</span>表示第<span class="math inline">\(l\)</span>个Resnet Block被保留的概率，如果<span class="math inline">\(b_l = 1\)</span>则退化到原始Resnet形式，如<span class="math inline">\(b_l = 0\)</span> 则相当于identity function， <span class="math display">\[\begin{equation}
  H_l = id(H_{l-1})
\end{equation}\]</span></p></li>
<li><p>The survival probabilities</p>
<p><span class="math inline">\(p_l\)</span>为新引入的模型参数，直观地相邻层的<span class="math inline">\(p_l\)</span>值应该很接近。实际上由于浅层用来提取特征并被深层使用，因此浅层被保留的概率应该更大。文章探讨了两种方式，一是设置<span class="math inline">\(p_l = p_L\)</span>，对所有层有均匀的保留概率，好处是只引入一个参数；另外一种方式是线性地衰减保留概率，如初始层<span class="math inline">\(p_0 = 1\)</span>，最深层为<span class="math inline">\(p_L\)</span>，则中间层<span class="math inline">\(l\)</span>被保留的概率为： <span class="math display">\[\begin{equation}
  p_l = 1 - \frac{l}{L}(1-p_L)
\end{equation}\]</span> Stochastic depth网络结构如下图：</p></li>
</ul>
<img src="/2019/03/23/stochastic-depth/stochastic_depth.png" title="stochastic depth">
<ul>
<li><p>Expected network depth</p>
<p>在训练中模型的实际有效深度为<span class="math inline">\(\hat{L}\)</span>，则其期望值<span class="math inline">\(E(\hat{L}) = \sum_{l=1}^{L}p_l\)</span>。在<span class="math inline">\(p_L = 0.5\)</span>的线性衰减策略下，<span class="math inline">\(E(\hat{L}) = (3L-1)/4\)</span>，对于文章使用的110层的网络，其<span class="math inline">\(L=54\)</span>，实际训练的有效深度约为40，这能显著缓解深层网络中的梯度消失与信息丢失问题。</p></li>
<li><p>Training time savings</p>
<p>当一个残差单元被drop时，就不需要为其进行前向及后向传播中的计算，这将大幅提升训练速度。对于上述<span class="math inline">\(p_L=0.5\)</span>的线性衰减网络而言，将节省约25%的训练时间。</p></li>
<li><p>Implicit model ensemble</p>
<p>以stochastic depth方式训练时相当于将不同深度的网络模型进行集成。对于<span class="math inline">\(L\)</span>个残差块的网络就有<span class="math inline">\(2^L\)</span>种不同的模型组合可能，每次训练一个batch时就从这<span class="math inline">\(2^L\)</span>中可能中选择一个网络，在测试时则对所有的网络进行平均。</p></li>
<li><p>Stochastic depth during testing</p>
<p>在测试时没有残差单元被丢弃，此时根据每个残差块参与训练的期望程度进行计算，测试时的前向传播如下： <span class="math display">\[\begin{equation}
  H_l^{Test} = ReLU(p_lf_l(H_{l-1}^{Test};W_l) + H_{l-1}^{Test})
\end{equation}\]</span></p></li>
</ul>
<h1 id="参考文献">参考文献</h1>
<blockquote>
Huang, G., Sun, Y., Liu, Z., Sedra, D., & Weinberger, K. Q. (2016, October). Deep networks with stochastic depth. In European conference on computer vision (pp. 646-661). Springer, Cham.
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/23/wrn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yilang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="爱学习">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/23/wrn/" class="post-title-link" itemprop="url">Wide Residual Networks</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-23 12:57:10 / 修改时间：12:38:02" itemprop="dateCreated datePublished" datetime="2019-03-23T12:57:10+08:00">2019-03-23</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/computer-vision/" itemprop="url" rel="index"><span itemprop="name">computer vision</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/computer-vision/classification/" itemprop="url" rel="index"><span itemprop="name">classification</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="介绍">介绍</h1>
<p>文章提出了通过扩展神经网络的宽度而不是深度也可以获得良好的性能，同时验证了残差单元中使用dropout的有效性。对于卷积神经网络而言，宽度指的是在每一层中使用的卷积核的数目，即通道这一维度的数目。</p>
<p>文章使用Resnet作为基本的参考网络结构，为了提高深度模型的表示能力，可以在每个残差单元中加入更多的层，扩大卷积层中卷积核的大小或扩宽卷积核。</p>
<p>文章使用以下符号标记：</p>
<ul>
<li><p>l: deepening factor，每个残差单元内卷积层的数量，对于basic版本Resnet是2，bottleneck版本是3。</p></li>
<li><p>k: widdening factor，每个卷积层处卷积核的数目是原始的多少倍，原始的网络结构都是1。</p></li>
<li><p>d: number of blocks, 基本残差单元的数量, 网络正是由一系列基本的残差单元堆叠而成。</p></li>
<li><p>B(M): B表示一个残差单元，M为此残差单元内的一系列卷积层，卷积核都是方块，残差单元内的卷积层都有相同的通道数。如B(3,3)就是原始basic版本Resnet残差单元，B(1,3,1)是对原始bottleneck版本Resnet残差单元的一个加强。(bottneck先1x1降维后1x1还原维度)</p></li>
<li><p>n: 网络中全部卷积层的数量。WRN-n-k表示一共有n层，widdening factor为k。</p></li>
</ul>
<p>文章通过实验发现网络层有相近数量的参数时模型输出有相近的性能。在网络有不同的深度时加宽网络能连续性地提升性能，同时加深网络和加宽网络会带来更多的参数进而需要更多的数据来防止拟合，同时在残差单元内的卷积层之间使用dropout是有效的，尽管已经使用了Batch Normalization。文章显示了使用16层加宽的网络能够获得媲美1000层的未加宽的性能，同时能够加快训练速度。</p>
<h1 id="参考文献">参考文献</h1>
<blockquote>
Zagoruyko, S., & Komodakis, N. (2016). Wide residual networks. arXiv preprint arXiv:1605.07146.
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/23/densenet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yilang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="爱学习">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/23/densenet/" class="post-title-link" itemprop="url">DenseNet:Densely Connected Convolutional Networks</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-23 12:47:52" itemprop="dateCreated datePublished" datetime="2019-03-23T12:47:52+08:00">2019-03-23</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-03-24 12:04:24" itemprop="dateModified" datetime="2019-03-24T12:04:24+08:00">2019-03-24</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/computer-vision/" itemprop="url" rel="index"><span itemprop="name">computer vision</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/computer-vision/classification/" itemprop="url" rel="index"><span itemprop="name">classification</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="介绍">介绍</h1>
<p>文章提出了DenseNet模型，其中Dense指神经网络层与层之间的连接。在常规的卷积神经网络中，L层的网络共有L个连接，在DenseNet中L层的网络有<span class="math inline">\(\frac{L(L+1)}{2}\)</span>个连接。在feature map大小匹配的层中，每一层都把其前面所有层的feature map作为其输入，也把其自己的feature map作为后续所有层的输入，Dense Block如下图所示：</p>
<img src="/2019/03/23/densenet/denseblock.png" width="400" title="dense block">
<p>DenseNet因其密集地连接方式，能够缓解梯度消失问题，强化特征传播与特征复用，事实上减少了参数数量。DenseNet中每一层都可以相对直接地获取从loss传来的梯度信号及原始输入信息，更好的信息与梯度流动让模型易于训练。DenseNet在CIFAR-10，CIFAR-100，SVHN，ImageNet等数据集上获得了state-of-the-art表现。</p>
<h1 id="densenet">DenseNet</h1>
<p>DenseNet由一系列Dense Block组成，每一个Dense Block内具有相同大小的feature map，不同的feature map大小的Dense Block之间通过Transition layer来连接，如下图所示：</p>
<img src="/2019/03/23/densenet/densenet.png" title="densenet">
<ul>
<li><p>Dense Block</p>
<p>对于ResNet的残差块有： <span class="math display">\[\begin{equation}
  x_l = H_l(x_{l-1}) + x_{l-1}
\end{equation}\]</span> 其中<span class="math inline">\(x_{l-1}\)</span>表示第<span class="math inline">\(l-1\)</span>层的输入，H是由Convluation, Batch Normalization, Relu组合而成的pre-activation残差函数(堆叠顺序为BN-Relu-Conv)。</p>
<p>Dense Block沿用Resnet的设计，区别为其接受前面所有层的feature map作为输入： <span class="math display">\[\begin{equation}
  x_l = H_l([x_0, x_1, ..., x_{l-1}])
\end{equation}\]</span></p>
<p>其中<span class="math inline">\([x_0, x_1, ..., x_{l-1}]\)</span>表示将0到<span class="math inline">\(l-1\)</span>层的feature map在通道维concat起来。</p></li>
<li><p>Transition Layer</p>
<p>Transition layer 连接不同feature map大小的Dense Block，其包含batch normalization, 1x1 conv layer和 2x2 average pooling。</p></li>
</ul>
<p>如果<span class="math inline">\(H_l\)</span>产生k个feature map，则第l层将有<span class="math inline">\(k_0+k(l-1)\)</span>个feature map作为输入，此处k为growth rate。DenseNet与此前的网络不同之处在于可以拥有非常narrow的层获得较好性能，如取k=12。</p>
<p>尽管每层只产生k个feature map，但其拥有非常多的输入feature map。同bottleneck版本resnet一样，可以先使用1x1卷积来降低维度提高计算效率，指定此种版本为DenseNet-B(BN-Relu-Conv(1x1)-BN-Relu-Conv(3x3))。同时也可以在transition layer降低维度，此种版本为DensetNet-C。</p>
<p>在ImageNet数据集上运行的类似Resnet结构的Densent组成如下表：</p>
<img src="/2019/03/23/densenet/architecture.png" title="architecture">
<p>使用DenseNet在ImageNet运行实验结果如下表，可见Denset有更高的参数使用效率，如DenseNet-201参数与Resnet-34相仿性能却与Resnet-101相差无几。 <img src="/2019/03/23/densenet/imagenet.png" title="imagenet"></p>
<h1 id="总结">总结</h1>
<p>DenseNet允许学到的feature map被后续所有层高效地访问，导致很好的特征复用与紧凑的模型。其中loss回传的梯度信息只需要经过很少的transition layer，有更短的路径，这相当于一种Deep Supervision。在Stochastic depth网络中，随机drop一些layer后也相当于创建了不同layer的直接连接，这表明DenseNet也是一种有效的正则化方式。总而言之，DenseNet可以通过更少的参数与计算量获得更佳的性能。</p>
<h1 id="参考文献">参考文献</h1>
<blockquote>
Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708).
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/22/identity-mapping/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yilang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="爱学习">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/22/identity-mapping/" class="post-title-link" itemprop="url">Identity Mappings in Deep Residual Networks</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-22 09:45:43 / 修改时间：12:05:15" itemprop="dateCreated datePublished" datetime="2019-03-22T09:45:43+08:00">2019-03-22</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/computer-vision/" itemprop="url" rel="index"><span itemprop="name">computer vision</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/computer-vision/classification/" itemprop="url" rel="index"><span itemprop="name">classification</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="介绍">介绍</h1>
<p>文章是对原始ResNet模型的改进，使得在前向与后向传播中信息能够更好地流动，进一步解决了深度学习模型训练中的退化问题。ResNet中Identity Mapping只存在shortcut connection中，改进后残差单元间传递的也是Identity Mapping。</p>
<p>ResNet是由一系列的残差单元堆叠而成，如下所示： <span class="math display">\[\begin{gather}
    y_l = h(x_l) + F(x_l, W_l),\\
    x_{l+1} = f(y_l)
\end{gather}\]</span> 其中<span class="math inline">\(x_l\)</span>是第<span class="math inline">\(l\)</span>个残差单元的输入，<span class="math inline">\(W_l = \{W_{l,k} | 1 \leq k \leq K\}\)</span>是第<span class="math inline">\(l\)</span>个残差单元的权重，<span class="math inline">\(K\)</span>是残差单元的层数(普通版<span class="math inline">\(K=2\)</span>,bottleneck版本<span class="math inline">\(K=3\)</span>)，<span class="math inline">\(F\)</span>是残差函数，<span class="math inline">\(f\)</span>是相加后的激活函数Relu，<span class="math inline">\(h\)</span>是identity mapping时有<span class="math inline">\(h(x_l) = x_l\)</span>。</p>
<p>如<span class="math inline">\(f\)</span>也是identity mapping,将(2)代入(1)可得： <span class="math display">\[\begin{equation}
    x_{l+1} = x_l + F(x_l, W_l)
\end{equation}\]</span> 迭代后对任意深层<span class="math inline">\(L\)</span>和浅层<span class="math inline">\(l\)</span>有： <span class="math display">\[\begin{equation}
    x_{L} = x_l + \sum_{i= l}^{L-1}{F(x_i, W_i) }
\end{equation}\]</span> 可以看出，任意深层<span class="math inline">\(L\)</span>可直接表示为任意浅层<span class="math inline">\(l\)</span>加上一个残差函数，可以很方便地传递梯度： <span class="math display">\[\begin{equation}
    \frac{\partial\epsilon}{\partial{x_l}} = \frac{\partial\epsilon}{\partial{x_L}}\frac{\partial{x_L}}{\partial{x_l}} = \frac{\partial\epsilon}{\partial{x_L}}\lgroup 1 + \sum_{i=l}^{L-1}{F(x_i, W_i)}\rgroup
\end{equation}\]</span> 其中<span class="math inline">\(\epsilon\)</span>表示loss，公式(5)说明了对深层<span class="math inline">\(L\)</span>的梯度可以直接传递到浅层<span class="math inline">\(l\)</span>。</p>
<p>下图显示了原版ResNet及改进后的区别。 <img src="/2019/03/22/identity-mapping/identity_mapping.png" width="400" title="identity mapping"></p>
<h1 id="identity-skip-connections">Identity Skip Connections</h1>
<p>为探讨Identity Skip Connection的重要性，先假定<span class="math inline">\(f\)</span>是identity，然后考察不同形式的skip connections。 对于取<span class="math inline">\(h(x_l) = \lambda_lx_l\)</span>，有 <span class="math display">\[\begin{equation}
    x_{l+1} = \lambda_lx_l + F(x_l, W_l)
\end{equation}\]</span> 迭代后对任意深层<span class="math inline">\(L\)</span>和浅层<span class="math inline">\(l\)</span>有： <span class="math display">\[\begin{equation}
    x_{L} = (\prod_{i=L}^{L-1}\lambda_i)x_l + \sum_{i= l}^{L-1}{\hat{F}(x_i, W_i)}
\end{equation}\]</span> 其中<span class="math inline">\(\hat{F}\)</span>为吸收了累乘因子的残差函数，进行求导有： <span class="math display">\[\begin{equation}
    \frac{\partial\epsilon}{\partial{x_l}} = \frac{\partial\epsilon}{\partial{x_L}}\lgroup (\prod_{i=L}^{L-1}\lambda_i) + \frac{\partial}{\partial{x_l}}\sum_{i=l}^{L-1}{\hat{F}(x_i, W_i)}\rgroup
\end{equation}\]</span></p>
<p>由于梯度中有<span class="math inline">\(\prod_{i=L}^{L-1}\lambda_i\)</span>，当网络变深时容易出现梯度消失与梯度爆炸问题。 下图所示为不同的skip connections 及其结果。</p>
<img src="/2019/03/22/identity-mapping/shortcut_connections.png" width="600" title="skip connections">
<img src="/2019/03/22/identity-mapping/shortcut_connections_result.png" width="600" title="skip connections result">
<h1 id="usage-of-activation-functions">Usage of Activation Functions</h1>
<p>接下来探讨Identity在<span class="math inline">\(f\)</span>方面的重要性，此时使用identity skip conntions，采用如下不同形式的残差块进行比较。</p>
<img src="/2019/03/22/identity-mapping/diff_activations.png" width="600" title="different activations">
<p>如上图(c)所示可实现naive版本的identity，但由于<span class="math inline">\(relu(x) = max(0, x)\)</span>总是输出非负数，会影响残差块的表达能力。 上图(d),(e)的pre-activation指将本来处于相加后使用的relu放置在残差块的最前面，且只影响残差块这一分支。此时有： <span class="math display">\[\begin{equation}
    x_{l+1} = x_l + F(\hat{f}(x_l), W_l)
\end{equation}\]</span> 形式如公式(4)，信息能够得到很好的传播。</p>
<p>事实上由于残差网络是由很多残差单元堆叠而成，after-addition activation和pre-activation是等价的，如下图所示：</p>
<img src="/2019/03/22/identity-mapping/pre_activation.png" width="600" title="pre activation">
<p>关于不同<span class="math inline">\(f\)</span>调整的结果见下表：</p>
<img src="/2019/03/22/identity-mapping/diff_activations_result.png" width="600" title="different activations result">
<h1 id="总结">总结</h1>
<p>本文提出了对ResNet结构的改进，使得信息能够更好地流通。主要是将原来会阻碍信息流动的addition之后的relu位置调到了残差函数的前面，使其不影响skip connection分支。</p>
<h1 id="参考文献">参考文献</h1>
<blockquote>
He, K., Zhang, X., Ren, S., & Sun, J. (2016, October). Identity mappings in deep residual networks. In European conference on computer vision (pp. 630-645). Springer, Cham.
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/21/batchnorm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yilang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="爱学习">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/21/batchnorm/" class="post-title-link" itemprop="url">Batch Normalization:Accelerating Deep Neural Network Training by Reducing Internal Covariate Shift</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-21 14:30:15 / 修改时间：16:39:34" itemprop="dateCreated datePublished" datetime="2019-03-21T14:30:15+08:00">2019-03-21</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/computer-vision/" itemprop="url" rel="index"><span itemprop="name">computer vision</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/computer-vision/classification/" itemprop="url" rel="index"><span itemprop="name">classification</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="介绍">介绍</h1>
<p>文章提出Batch Normalization，解决了网络中每层处理单元的输入分布随参数更新变化的问题，也即internal covariate shift，使得能够使用较大的学习率，不用过多关注初始化，减少了使用dropout的必要，能够加快网络的训练和获得更好的性能。</p>
<p>在训练神经网络时，浅层参数的更新会影响后续层的输入，导致后续层需要适应其输入分布的变化。对于sigmoid这类饱和激活函数而言，当其输入数据落入其饱和区时会导致梯度接近于0，参数更新变小，影响网络收敛速度。</p>
<p>通常我们使用白化操作对输入数据进行预处理，主要是PCA白化和ZCA白化，通过白化操作会去除特征之间的相关性，使得输入分布有相同的均值和方差，但对网络中的每一层数据都进行白化操作成本太高，因此提出了batch normalization，使得网络中每一层处理单元的输入有相同的分布。</p>
<h1 id="batch-normalization">Batch Normalization</h1>
<p>Batch normalization对同一batch内网络中的每一层的每一个特征，首先进行归一化至均值为0，标准差为1，对于d维输入$x = (x^{(1)}, x^{(2)}, ..., x^{(d)}) $ 有：</p>
<center>
<span class="math inline">\({\hat{x}}^{(k)} = \frac{ x^{(k)} - E[x^{(k)}] }{\sqrt{Var[x^{(k)}]}}\)</span>
</center>
<p>然后还原数据的表达能力：</p>
<center>
<span class="math inline">\({\hat{y}}^{(k)} = \gamma^{(k)} \hat{x}^{(k)} + \beta^{(k)}\)</span>
</center>
<p>其中<span class="math inline">\(\gamma^{(k)},\beta^{(k)}\)</span>是第k处需要学习的一对参数。经过上述处理后，对应k处数据有稳定的均值为<span class="math inline">\(\beta^{(k)}\)</span>，标准差为<span class="math inline">\(\gamma^{(k)}\)</span>的分布。 最终将网络中每一层中原始的数据<span class="math inline">\(x^{(k)}\)</span>替换为<span class="math inline">\(y^{(k)}\)</span>。算法描述见下图：</p>
<img src="/2019/03/21/batchnorm/batch_norm_single.png" width="600" title="batch norm single">
<p>对于全连接层，针对每个激活的神经元进行处理。对于卷积层，针对每一个feature map学习一对<span class="math inline">\(\beta^{(k)}\)</span>，<span class="math inline">\(\gamma^{(k)}\)</span>。</p>
<p>在测试时，网络中每一层的输入分布已经固定，其均值与标准差通过moving average的方式由训练时求得的<span class="math inline">\(\beta^{(k)}\)</span>，<span class="math inline">\(\gamma^{(k)}\)</span>计算而来，整体流程如下：</p>
<img src="/2019/03/21/batchnorm/batch_norm_all.png" width="600" title="batch norm all">
<h1 id="总结">总结</h1>
<p>Batch Normalization加快了训练速度，能够获取更好的性能。BN-Inception是增加了batch normaliztion的inception模型，也即使Inception-V2，在ImageNet上获得了top-5 error为4.82%。</p>
<h1 id="参考文献">参考文献</h1>
<blockquote>
Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167.
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/19/resnet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yilang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="爱学习">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/19/resnet/" class="post-title-link" itemprop="url">Resnet:Deep Residual Learning for Image Recognition</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-19 20:55:40" itemprop="dateCreated datePublished" datetime="2019-03-19T20:55:40+08:00">2019-03-19</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-03-20 16:21:44" itemprop="dateModified" datetime="2019-03-20T16:21:44+08:00">2019-03-20</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/computer-vision/" itemprop="url" rel="index"><span itemprop="name">computer vision</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/computer-vision/classification/" itemprop="url" rel="index"><span itemprop="name">classification</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="介绍">介绍</h1>
<p>文章提出残差网络使得能够更容易地训练出更深层次的神经网络，Resnet模型给出了高至152层的网络，但却比VGG有更小的复杂度。Resnet模型横扫了ILSVRC-2015，COCO-2015上的分类、检测、分割等比赛任务的冠军。</p>
<p>更深的神经网络可能带来更好的性能，但却存在难以训练的退化问题，这并不是因为更深的网络导致了过拟合，而是训练误差都不能得到很好地下降，如下图所示：</p>
<img src="/2019/03/19/resnet/degradation.png" width="600" title="degradation">
<p>退化问题说明了深度模型训练的困难。从理论分析来看，考虑一个浅层网络和在其之上加了更多层的深层网络，对于深层网络可以把额外增加的层训练为恒等映射，其余部分从浅层网络拷贝过来，那么对于深层网络就应该存在方案使得其训练误差可以比相应的浅层网络要小。实际训练网络的情况并非如此，本文使用残差学习解决这个问题。</p>
<h1 id="deep-residual-learning">Deep Residual Learning</h1>
<h2 id="residual-learning">Residual Learning</h2>
<p>假设<span class="math inline">\(H(x)\)</span>是要通过堆叠的网络层学习的从输入到输出的映射，<span class="math inline">\(x\)</span>表示输入，如果假定多层网络能够渐近地学习到<span class="math inline">\(H(x)\)</span>，那么其也应该能学习到<span class="math inline">\(F(x)=H(x)-x\)</span>。通过残差方式只需要学习<span class="math inline">\(F(x)\)</span>，原始问题为<span class="math inline">\(F(x)+x\)</span>，两种方式的学习难易程度却有很大的不同。</p>
<img src="/2019/03/19/resnet/residual.png" width="400" title="degradation">
<h2 id="identity-mapping-by-shortcuts">Identity Mapping by Shortcuts</h2>
<p>残差学习可形式化地表达为<span class="math inline">\(y=F(x,{W_i})+x\)</span>，其中<span class="math inline">\(x,y\)</span>是当前整个块的输入和输出，<span class="math inline">\(F(x,{W_i})\)</span>表示需要学习的残差映射。如上图所示为一个残差building block，包含两层，则<span class="math inline">\(F=W_2\sigma(W_1x)\)</span>，其中<span class="math inline">\(\sigma\)</span>表示Relu函数。由公式可以看出，残差方式没有带来额外的参数，也基本没有带来更多地运算，因此可以公平地与相应地plain network进行比较。当<span class="math inline">\(x\)</span>与<span class="math inline">\(F\)</span>有相同的维度时，直接进行element-wise相加即可，当维度不同时可对<span class="math inline">\(x\)</span>投影到匹配的维度后再进行相加，此时有<span class="math inline">\(y=F(x,{W_i})+W_sx\)</span>。当维度本就相同时也可使用方阵形式的<span class="math inline">\(W_s\)</span>。</p>
<h2 id="network-architecture">Network Architecture</h2>
<ul>
<li><p>plain network</p>
<p>plain network如同VGG一样堆叠3x3卷积网络，遵循两个设计原则：</p>
<ul>
<li>对于输出的feature map大小一样的层有相同数量的卷积核。</li>
<li>如果输出的feature map大小减半，则卷积核的数量翻倍以保持每层的复杂度。</li>
</ul></li>
<li><p>residual network</p>
<p>residual network在前述的plain network上插入shortcut connection。当输入与输出维度相同时，可直接使用identity shortcut。当维度不同时，可仍选择shortcut mapping但进行额外补0以使维度匹配，或者使用projection shortcut。当shortcut connection跨越不同大小的feature map时，projection使用步长为2。</p></li>
<li><p>bottleneck</p>
<p>当配置的网络较深时存在参数过多的问题，因此可引入1x1的卷积核来降低维度减少计算量。具体来说，2层的3x3残差块中可由1x1,3x3,1x1堆叠的三层块代替，前面的1x1卷积用来降低维度，后面的1x1用来提升维度以使输入输出维度匹配，如下图所示：</p></li>
</ul>
<img src="/2019/03/19/resnet/bottleneck.png" width="400" title="bottleneck">
<ul>
<li><p>architecture</p>
<p>常用resnet有18，34，50，101及152层，其中50层及以上使用bottleneck版本的块。resnet首先使用步长为2的7x7卷积，然后包含四组不同feature map大小的堆叠的残差块，因此最终feature map的大小为输入的1/32。resnet中无全连接层，在最终的feature map上执行全局池化后输入softmax进行分类。不同深度的resnet配置如下表：</p></li>
</ul>
<img src="/2019/03/19/resnet/resnet.png" title="resnet">
<h1 id="参考文献">参考文献</h1>
<blockquote>
He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/19/googlenet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yilang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="爱学习">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/19/googlenet/" class="post-title-link" itemprop="url">GoogLeNet:Going Deeper with Convolutions</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-19 16:25:44 / 修改时间：18:32:03" itemprop="dateCreated datePublished" datetime="2019-03-19T16:25:44+08:00">2019-03-19</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/computer-vision/" itemprop="url" rel="index"><span itemprop="name">computer vision</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/computer-vision/classification/" itemprop="url" rel="index"><span itemprop="name">classification</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="介绍">介绍</h1>
<p>GoogLeNet模型由Google团队提出，在ILSVRC-2014比赛中获得分类任务第一名。GoogLeNet设计了Inception模块，也即Inception-V1版本。</p>
<h1 id="inception">Inception</h1>
<p>Inception模块使用不同大小的卷积核提取特征并组合在一起实现不同尺度的特征融合。 其中1x1,3x3,5x5卷积核使用的padding分别为0,1,2,保证了卷积前后feature map大小完全一致，3x3 max pooling步长为1，可以直接在通道维度拼接不同卷积的结果。</p>
<p>为减少计算量，在3x3及5x5卷积核前引入1x1卷积调整通道数目降低维度, 3x3 max pooling后也引入1x1卷积降维， 如下图所示。</p>
<img src="/2019/03/19/googlenet/inception.png" width="400" title="inception">
<h1 id="googlenet">GoogleNet</h1>
<p>GoogLeNet将相同结构的Inception模块堆叠起来，共22层，模型结构如下：</p>
<p>其中输入为224x224x3, 首先仍然使用传统7x7卷积，然后依据feature map大小引入三个层次的Inception模块，分别为inception(3a,3b,4a,4b,4c,4d,4e,5a,5b)，前面的数字表示相对于原始输入特征大小缩小了2的几次方倍，字母a,b等表示级联的顺序，前一个Inception模块融合后的结果将作为下一个Inception模块的输入。不同层次的Inception模块间通过步长为2的max pooling特征图大小减半。最终通过softmax实现多分类任务。</p>
<img src="/2019/03/19/googlenet/googlenet-arch.png" title="This is googlenet architecture">
<p>因网络较深，不利于loss反向传递更新参数，在Inception(4a, 4d) 后引入了两个小的网络进行辅助训练。辅助网络同样进行多分类，不参与模型测试。 模型整体流程见下图： <img src="/2019/03/19/googlenet/googlenet.jpg" title="This is googlenet architecture"></p>
<h1 id="结果">结果</h1>
<p>在ILSVRC-2014分类任务上获得冠军，top-5 error 为6.67%。</p>
<h1 id="参考文献">参考文献</h1>
<blockquote>
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9).
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/19/vgg/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yilang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="爱学习">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/19/vgg/" class="post-title-link" itemprop="url">VGG:Very deep convolutional networks for large-scale image recognition</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-19 14:25:31 / 修改时间：15:43:13" itemprop="dateCreated datePublished" datetime="2019-03-19T14:25:31+08:00">2019-03-19</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/computer-vision/" itemprop="url" rel="index"><span itemprop="name">computer vision</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/computer-vision/classification/" itemprop="url" rel="index"><span itemprop="name">classification</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="介绍">介绍</h1>
<p>VGG模型来自牛津大学Visual Geometry Group, 其堆叠一系列3x3卷积核，在ILSVRC-2014获得定位任务第一名，分类任务第二名。</p>
<h1 id="架构">架构</h1>
<p>模型仅使用3x3及1x1大小的卷积核，3x3为可捕获上下左右信息的最小卷积窗口。所有的3x3卷积有padding=1，步长为1，因此输入输出的特征图大小不变。</p>
<p>池化层窗口大小为2x2,步长为2，每次池化后特征图大小减半。紧接池化后的卷积通道数翻倍，直至最后为512，保持运算的均衡。</p>
<p>两层连续堆叠的3x3卷积与直接5x5卷积有大小相同的感受野，三层连续堆叠的3x3卷积与直接7x7卷积有大小相同的感受野，因此可使用堆叠的较小卷积核来替代较大的卷积。</p>
<ul>
<li><p>堆叠的小卷积核相较于单层大卷积核有更强的判别能力。</p>
<p>因堆叠后层数更深，卷积之间有更多的非线性变换(relu), 可增加模型的判别能力。</p></li>
<li><p>堆叠的小卷积核相较于单层大卷积核有更少的参数。</p>
<p>如3个3x3通道为C的卷积核参数量为<span class="math inline">\(3(3^2C^2)=27C^2\)</span>, 而1个7x7通道为C的卷积核参数量为<span class="math inline">\(7^2C^2=49C^2\)</span>。</p></li>
</ul>
<p>不同深度的模型配置如下表：</p>
<img src="/2019/03/19/vgg/vgg.png" title="This is VGG architecture">
<h1 id="实验">实验</h1>
<ul>
<li><p>多尺度训练</p></li>
<li><p>多模型集成</p></li>
</ul>
<h1 id="参考文献">参考文献</h1>
<blockquote>
Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/13/surf/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yilang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="爱学习">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/13/surf/" class="post-title-link" itemprop="url">surf</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-13 11:59:10 / 修改时间：12:20:28" itemprop="dateCreated datePublished" datetime="2019-03-13T11:59:10+08:00">2019-03-13</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/computer-vision/" itemprop="url" rel="index"><span itemprop="name">computer vision</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/computer-vision/features/" itemprop="url" rel="index"><span itemprop="name">features</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">yilang</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">64</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">6</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">13</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yilang</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.0.1</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=7.0.1"></script>

  <script src="/js/src/motion.js?v=7.0.1"></script>



  
  


  <script src="/js/src/affix.js?v=7.0.1"></script>

  <script src="/js/src/schemes/pisces.js?v=7.0.1"></script>




  

  


  <script src="/js/src/next-boot.js?v=7.0.1"></script>


  

  

  

  



  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  
  
    
      
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
  overflow-x: scroll;
  overflow-y: hidden;
}
</style>

    
  


  

  

  

  

  

  

  

  

  

</body>
</html>
